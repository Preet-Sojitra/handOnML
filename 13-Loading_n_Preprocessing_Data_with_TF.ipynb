{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57b95104-bc2d-4d4b-b221-a1bfa1117209",
   "metadata": {},
   "source": [
    "# Loading and Preprocessing Data with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "16833ab2-1ec5-4453-878f-bc6561934634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb398cd-51d0-41de-8cda-520d2ae61bfe",
   "metadata": {},
   "source": [
    "## The Data API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c71574-1510-4009-8a12-81aad35c7622",
   "metadata": {},
   "source": [
    "The whole Data api revolves around the concept of *datasets*. Usually, we will use datasets that gradually read data from disk, but from simplicity let's create a dataset entirely in RAM using `tf.data.Dataset.from_tensor_slices()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17a2f9ed-7d28-4822-9dec-8f136cc2c95d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.range(10) # any data tensor\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8c0f07-41c7-4e0b-ba1b-b4aadd3524bc",
   "metadata": {},
   "source": [
    "The `from_tensor_slices()` function takes a tensor and creates a `tf.data.Dataset` whose elements are all slices of `X` (along the first dimension), so this dataset contains 10 items: tensors 0, 1, 2,...,9. In this case we would have obtained the same dataset if we had used `tf.data.Dataset.range(10)`\n",
    "\n",
    "We can simply iterate over a dataset's items like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "351abba5-18ff-4d3b-8413-dbc8b5ddbf4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6103af0-3e84-4ca3-8396-f8ce61558b86",
   "metadata": {},
   "source": [
    "### Chaining Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6a2a8b-c918-4953-b4d4-832f2f9f4918",
   "metadata": {},
   "source": [
    "Once we have dataset, we can apply all sort of transformations to it by calling its transformation methods. Each method returns a new dataset, so we can chain transformations like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9092178d-d75f-47e3-8f77-c738f53d5903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.repeat(3).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f907b2c4-4f67-4d4d-876a-e6331fc9d4dc",
   "metadata": {},
   "source": [
    "In this example, we first call `repeat()` method on the original dataset, and it returns a new dataset that will repeat the items in the original dataset three times. Then we call the `batch()` method on this new dataset, and again creates a new dataset. This one will group the items of the previous dataset in batches of seven items. Finally, we can iterate over the items of this final dataset.\n",
    "\n",
    "As we can see, the `batch()` method had to output a final batch of size two instead of seven, but we can call it with a `drop_remainder=True` if we want it to drop this final batch so that all batches have the exact same size. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda4554f-5ed1-4c3d-b60a-d241e9a69966",
   "metadata": {},
   "source": [
    "We can also transform the items by calling the `map()` method. For ex., this creates a new dataset with all items doubled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d5af453-eae7-42d9-a4c7-6614c59ffbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 0  2  4  6  8 10 12], shape=(7,), dtype=int32)\n",
      "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int32)\n",
      "tf.Tensor([16 18], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(lambda x: x*2)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b1ab84-28aa-4910-a9cc-13da9397a2ba",
   "metadata": {},
   "source": [
    "This function is the one we will call to apply any preprocessing we want to our data. Sometimes this will include computations that can be quite intensive, such as reshaping or rotating an image, so we will usually want to spawn multiple threads to speed things up: it's as simple as setting the\n",
    "`num_parallel_calls` argument. \n",
    "\n",
    "**NOTE**:\n",
    "The function we pass to the `map()` must be convertible to TF Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bd2cbf-d30f-4fd1-9a25-2e892ad11a24",
   "metadata": {},
   "source": [
    "While `map()` method applies transformation to each item, the `apply()` method applies a transformation to the dataset as a whole. \n",
    "\n",
    "For ex., the `unbatch()` function to the dataset will create a new dataset where each item will be single-integer tensor instead of batch of seven integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "680dcddc-6931-4543-8f5c-eedff886978f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(10, shape=(), dtype=int32)\n",
      "tf.Tensor(12, shape=(), dtype=int32)\n",
      "tf.Tensor(14, shape=(), dtype=int32)\n",
      "tf.Tensor(16, shape=(), dtype=int32)\n",
      "tf.Tensor(18, shape=(), dtype=int32)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(10, shape=(), dtype=int32)\n",
      "tf.Tensor(12, shape=(), dtype=int32)\n",
      "tf.Tensor(14, shape=(), dtype=int32)\n",
      "tf.Tensor(16, shape=(), dtype=int32)\n",
      "tf.Tensor(18, shape=(), dtype=int32)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(10, shape=(), dtype=int32)\n",
      "tf.Tensor(12, shape=(), dtype=int32)\n",
      "tf.Tensor(14, shape=(), dtype=int32)\n",
      "tf.Tensor(16, shape=(), dtype=int32)\n",
      "tf.Tensor(18, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.unbatch()\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dae0e3a-7246-465a-949f-a7e45ead069b",
   "metadata": {},
   "source": [
    "It is also possible to simply filter the dataset using the `filter()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "940e787c-458c-4d32-968e-56be7edd2387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.filter(lambda x: x < 10)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a012fb54-73b2-44f9-b467-b48459356787",
   "metadata": {},
   "source": [
    "We will often want to look at just few items from a dataset. We can use `take()` method for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dfeb3cca-4471-49cf-b0ef-e25a32b15797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset.take(3):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8f3cd4-58a6-48df-b2d8-bb24df35675e",
   "metadata": {},
   "source": [
    "### Shuffling the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989c0074-5406-4775-8c18-35dd901bca00",
   "metadata": {},
   "source": [
    "Gradient Descent works best when the instances in the training set are independent and identically distributed. A simple way to ensure this is to shuffle the instances using the `shuffle()` method.  For ex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4238aa6e-3cc6-478d-8b0b-d205971fbf5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 2 3 6 7 9 4], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 0 1 1 8 6 5], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 8 7 1 2 3 0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 4 2 7 8 9 9], shape=(7,), dtype=int64)\n",
      "tf.Tensor([3 6], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(10).repeat(3) # 0 to 9, 3 times\n",
    "dataset = dataset.shuffle(buffer_size=5, seed=42).batch(7) # we must specify the buffer size, and it is imp to make it large enough, or else shuffling will not be very effective. \n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6c2a72-b363-4773-ae3a-7ba7e21f2638",
   "metadata": {},
   "source": [
    "**TIP:**\n",
    "\n",
    "If we call `repeat()` on a shuffled dataset, by default it will generate a new order at every iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d04bc1-80b6-4e85-b1b5-aeaaecc810e4",
   "metadata": {},
   "source": [
    "For large datasets that does not fit in memory, this simple shuffling-buffer approach may not be sufficient, since the buffer will be small compared to dataset. One solution is to shuffle the source data itself. This will definitely improve the shuffling a lot. Even if the data is shuffled, we usually want to shuffle it more. To shuffle the instances some more, a common approach is to split source data into multiple files, then read them in random order during training. However, instances located in same file will still end up close together. To avoid this, we can pick multiple files randomly and read them simultaneously, interleaving records. Then on top of that we can add a shuffling buffer using `shuffle()` method. Let's see how it works:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595005da-b980-4b7c-9a0e-10d32dade3a0",
   "metadata": {},
   "source": [
    "Let's start by loading and preparing California dataset. We will first load it, split it intro training, validation and test set and finally we scale it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3570effe-b24f-4cbb-9937-30afccf8ded7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data,\n",
    "                                                              housing.target.reshape(-1,1),\n",
    "                                                             random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_mean = scaler.mean_\n",
    "X_std = scaler.scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a87d06b-8908-4118-af5e-4867594083de",
   "metadata": {},
   "source": [
    "For very large dataset that does not fit in memory, we will typically need to split it into many files first, then have TF read these files in parallel. To demonstrate this, let's start by splitting the housing dataset and save it in 20 csv files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84dec3e5-1838-482b-9c15-1ecff79666b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_multiple_files(data, name_prefix, header=None, n_parts=10):\n",
    "    housing_dir = os.path.join(os.curdir, \"datasets\", \"california_housing\")\n",
    "    # print(housing_dir)\n",
    "    os.makedirs(housing_dir, exist_ok=True)\n",
    "    path_format = os.path.join(housing_dir, \"my_{}_{:02d}.csv\")\n",
    "    \n",
    "    file_paths = []\n",
    "    m = len(data)\n",
    "    for file_idx, row_indices in enumerate(np.array_split(np.arange(m), n_parts)):\n",
    "        part_csv = path_format.format(name_prefix, file_idx)\n",
    "        file_paths.append(part_csv)\n",
    "        with open (part_csv, \"wt\", encoding=\"utf-8\") as f:\n",
    "            if header is not None:\n",
    "                f.write(header)\n",
    "                f.write(\"\\n\")\n",
    "            for row_idx in row_indices:\n",
    "                \"\"\"\n",
    "                Python repr() function returns a printable representation of the object by converting\n",
    "                that object to a string.\n",
    "                \"\"\"\n",
    "                f.write(\",\".join([repr(col) for col in data[row_idx]]))\n",
    "                f.write(\"\\n\")\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c9df56a-d07f-4255-9480-a72133ea3e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.c_[X_train, y_train]\n",
    "valid_data = np.c_[X_valid, y_valid]\n",
    "test_data = np.c_[X_test, y_test]\n",
    "header_cols = housing.feature_names + [\"MedianHouseValue\"]\n",
    "header = \",\".join(header_cols)\n",
    "\n",
    "train_filepaths = save_to_multiple_files(train_data, \"train\", header, n_parts=20)\n",
    "valid_filepaths = save_to_multiple_files(valid_data, \"valid\", header, n_parts=10)\n",
    "test_filepaths = save_to_multiple_files(test_data, \"test\", header, n_parts=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "423f2f23-a409-43bf-80e7-0bc766c90b0d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./datasets/california_housing/my_train_00.csv',\n",
       " './datasets/california_housing/my_train_01.csv',\n",
       " './datasets/california_housing/my_train_02.csv',\n",
       " './datasets/california_housing/my_train_03.csv',\n",
       " './datasets/california_housing/my_train_04.csv',\n",
       " './datasets/california_housing/my_train_05.csv',\n",
       " './datasets/california_housing/my_train_06.csv',\n",
       " './datasets/california_housing/my_train_07.csv',\n",
       " './datasets/california_housing/my_train_08.csv',\n",
       " './datasets/california_housing/my_train_09.csv',\n",
       " './datasets/california_housing/my_train_10.csv',\n",
       " './datasets/california_housing/my_train_11.csv',\n",
       " './datasets/california_housing/my_train_12.csv',\n",
       " './datasets/california_housing/my_train_13.csv',\n",
       " './datasets/california_housing/my_train_14.csv',\n",
       " './datasets/california_housing/my_train_15.csv',\n",
       " './datasets/california_housing/my_train_16.csv',\n",
       " './datasets/california_housing/my_train_17.csv',\n",
       " './datasets/california_housing/my_train_18.csv',\n",
       " './datasets/california_housing/my_train_19.csv']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_filepaths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7046dd4-c59b-4c17-a095-e1dca664ec3f",
   "metadata": {},
   "source": [
    "Okay, now let's look at the first few lines of one of these csv files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c3ace728-0e7e-447a-92a7-e3022cb59058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>MedianHouseValue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.5214</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.049945</td>\n",
       "      <td>1.106548</td>\n",
       "      <td>1447.0</td>\n",
       "      <td>1.605993</td>\n",
       "      <td>37.63</td>\n",
       "      <td>-122.43</td>\n",
       "      <td>1.442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.3275</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.490060</td>\n",
       "      <td>0.991054</td>\n",
       "      <td>3464.0</td>\n",
       "      <td>3.443340</td>\n",
       "      <td>33.69</td>\n",
       "      <td>-117.39</td>\n",
       "      <td>1.687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.1000</td>\n",
       "      <td>29.0</td>\n",
       "      <td>7.542373</td>\n",
       "      <td>1.591525</td>\n",
       "      <td>1328.0</td>\n",
       "      <td>2.250847</td>\n",
       "      <td>38.44</td>\n",
       "      <td>-122.98</td>\n",
       "      <td>1.621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.1736</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.289003</td>\n",
       "      <td>0.997442</td>\n",
       "      <td>1054.0</td>\n",
       "      <td>2.695652</td>\n",
       "      <td>33.55</td>\n",
       "      <td>-117.70</td>\n",
       "      <td>2.621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0549</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.312457</td>\n",
       "      <td>1.085092</td>\n",
       "      <td>3297.0</td>\n",
       "      <td>2.244384</td>\n",
       "      <td>33.93</td>\n",
       "      <td>-116.93</td>\n",
       "      <td>0.956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  3.5214      15.0  3.049945   1.106548      1447.0  1.605993     37.63   \n",
       "1  5.3275       5.0  6.490060   0.991054      3464.0  3.443340     33.69   \n",
       "2  3.1000      29.0  7.542373   1.591525      1328.0  2.250847     38.44   \n",
       "3  7.1736      12.0  6.289003   0.997442      1054.0  2.695652     33.55   \n",
       "4  2.0549      13.0  5.312457   1.085092      3297.0  2.244384     33.93   \n",
       "\n",
       "   Longitude  MedianHouseValue  \n",
       "0    -122.43             1.442  \n",
       "1    -117.39             1.687  \n",
       "2    -122.98             1.621  \n",
       "3    -117.70             2.621  \n",
       "4    -116.93             0.956  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(train_filepaths[0]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5329e7a8-b218-4111-98ff-3ecda130b0aa",
   "metadata": {},
   "source": [
    "Or in text mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "00a28eef-aa72-495f-8934-5c9d38cd3c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n",
      "3.5214,15.0,3.0499445061043287,1.106548279689234,1447.0,1.6059933407325193,37.63,-122.43,1.442\n",
      "5.3275,5.0,6.490059642147117,0.9910536779324056,3464.0,3.4433399602385686,33.69,-117.39,1.687\n",
      "3.1,29.0,7.5423728813559325,1.5915254237288134,1328.0,2.2508474576271187,38.44,-122.98,1.621\n",
      "7.1736,12.0,6.289002557544757,0.9974424552429667,1054.0,2.6956521739130435,33.55,-117.7,2.621\n"
     ]
    }
   ],
   "source": [
    "with open(train_filepaths[0]) as f:\n",
    "    for i in range(5):\n",
    "        print(f.readline(), end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff09b09c-1e9f-427a-87ca-3f51bb4c010a",
   "metadata": {},
   "source": [
    "#### Interleaving lines from multiple files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed97b32-ae3a-4dec-8cee-b732da29e88e",
   "metadata": {},
   "source": [
    "Now let's create a dataset containing only these train file paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "88b1f23a-f040-4632-8235-8af55f505b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db970b2-6713-4519-bb24-ca46b5e44dcf",
   "metadata": {},
   "source": [
    "By defult `list_files()` returns a dataset that shuffles the file paths. In general this is good, but we can set `shuffle=False` if we do not want that for some reason."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d679da-3b2e-475c-a71c-0303283f8a89",
   "metadata": {},
   "source": [
    "Now we can call the `interleave()` method to read from five files at a time and interleave thier lines (skipping the first line of each file, which is header row, using the `skip()` method):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5b730126-d495-49ce-9b57-48ea36553e96",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'./datasets/california_housing/my_train_05.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./datasets/california_housing/my_train_16.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./datasets/california_housing/my_train_01.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./datasets/california_housing/my_train_17.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./datasets/california_housing/my_train_00.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./datasets/california_housing/my_train_14.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./datasets/california_housing/my_train_10.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./datasets/california_housing/my_train_02.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./datasets/california_housing/my_train_12.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./datasets/california_housing/my_train_19.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./datasets/california_housing/my_train_07.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./datasets/california_housing/my_train_09.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./datasets/california_housing/my_train_13.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./datasets/california_housing/my_train_15.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./datasets/california_housing/my_train_11.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./datasets/california_housing/my_train_18.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./datasets/california_housing/my_train_04.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./datasets/california_housing/my_train_06.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./datasets/california_housing/my_train_03.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./datasets/california_housing/my_train_08.csv', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for filepath in filepath_dataset:\n",
    "    print(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5819c723-bea9-469a-aac1-b13c2d9a5770",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_readers = 5\n",
    "\"\"\"\n",
    "The tf.data.TextLineDataset loads text from text files and creates a dataset where each line of the files becomes an element of the dataset.\n",
    "\"\"\"\n",
    "dataset = filepath_dataset.interleave(lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "                                  cycle_length=n_readers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fef876-2af2-4c02-9879-54d4979689a2",
   "metadata": {},
   "source": [
    "The `interleave()` method will create a dataset that will pull five file path from the `filepath_dataset`, and for each one it will call the function we gave it (a lambda in this example), to create a new dataset (in this case a `TextLineDataset`). \n",
    "\n",
    "To be clear, at this stage there will be seven datasets in all: the filepath dataset, the interleave dataset, and the five `TextLineDataset` created internally by the interleave dataset. When we interate over the interleave dataset, it will cycle through these five `TextLineDatasets`, reading one line at a time from each until all datasets are out of items. Then it will get next five file paths from the `filepath_datasets` and interleave them the same way, and so on until it runs out of file paths. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24895a8-abf3-411c-8a3c-6092b5f890e0",
   "metadata": {},
   "source": [
    "**TIP:**\n",
    "\n",
    "For interleaving to work best, it is preferable to have files of identical lengths; otherwise the ends of the longest files will not be interleaved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dd5d81-e905-4b08-8e06-5f8476122e79",
   "metadata": {},
   "source": [
    "By default, `interleave()` does not use parallelism; it just reads one line at a time from each line, sequentially. If we want it to actually read files in parallel, we can set the `num_parallel_calls` argument to the number of threads we want. We can even set  it to `tf.data.AUTOTUNE` to make tf choose the right number of threads dynamically based on the available CPU. \n",
    "\n",
    "Let's look at what the dataset contains now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "598228f4-327c-44e9-8222-7bdacd2975be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'4.1812,52.0,5.701388888888889,0.9965277777777778,692.0,2.4027777777777777,33.73,-118.31,3.215'\n",
      "b'3.226,52.0,5.372469635627531,0.9473684210526315,1157.0,2.3421052631578947,37.96,-121.31,1.076'\n",
      "b'4.2708,45.0,5.121387283236994,0.953757225433526,492.0,2.8439306358381504,37.48,-122.19,2.67'\n",
      "b'3.5214,15.0,3.0499445061043287,1.106548279689234,1447.0,1.6059933407325193,37.63,-122.43,1.442'\n",
      "b'3.0217,22.0,4.983870967741935,1.1008064516129032,615.0,2.4798387096774195,38.76,-120.6,1.069'\n"
     ]
    }
   ],
   "source": [
    "for line in dataset.take(5):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1697b8f-59ce-4a16-979b-8973d214c9a5",
   "metadata": {},
   "source": [
    "Looks good! But as we can see, these are just byte strings; we need to parse them and scale the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "05cfecc2-8468-4895-95e0-f679137a6b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=int32, numpy=1>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=2.0>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=3.0>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'4'>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=5.0>]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record_defaults = [0, np.nan, tf.constant(np.nan, dtype=np.float64), \"Hello\", tf.constant([])]\n",
    "\"\"\"\n",
    "tf.io.decode_csv : Convert CSV records to tensors. Each column maps to one tensor.\n",
    "\n",
    "record_defaults = A list of Tensor objects with specific types. \n",
    "\"\"\"\n",
    "parsed_fields = tf.io.decode_csv(\"1,2,3,4,5\", record_defaults)\n",
    "parsed_fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de765807-6d12-429b-8137-55c7c5ca5933",
   "metadata": {},
   "source": [
    "Notice that in above, field 4 is interpreted as string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0c8219a7-2aa1-4854-a52b-da2845d2b50c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=int32, numpy=0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=nan>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=nan>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'Hello'>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=5.0>]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_fields = tf.io.decode_csv(\",,,,5\", record_defaults)\n",
    "parsed_fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580bd9e9-0097-4055-9821-44ee55775664",
   "metadata": {},
   "source": [
    "Notice that all the missing values are replaced by default values.\n",
    "\n",
    "Also the number of fields should match exactly the number of fields in record_defaults"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e780ac4f-99d0-4b1f-bab2-636b8cbb11fb",
   "metadata": {},
   "source": [
    "### Preprocessing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a64c32-bb54-4ba6-9339-61bdbe3abc9f",
   "metadata": {},
   "source": [
    "Now let's implement a function that perfroms this preprocessing (parsing the csv and scaling them):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fde3a569-e8a1-41ad-afc1-bc1a4a59d89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = X_train.shape[-1]\n",
    "\n",
    "def preprocess(line):\n",
    "    defaults = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "    fields = tf.io.decode_csv(line, record_defaults=defaults)\n",
    "    # print(fields)\n",
    "    # fields is a list of tensors where each tensor contains one feature value. So we need to use tf.stack to make it to array\n",
    "    x = tf.stack(fields[:-1])\n",
    "    # print(x)\n",
    "    y = tf.stack(fields[-1:])\n",
    "    return (x-X_mean) / X_std, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "16d00b13-2f6f-41a4-adb1-91c38edf4b12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
       " array([ 0.15159765,  1.8491895 ,  0.09624147, -0.22151624, -0.66828614,\n",
       "        -0.23549314, -0.89780813,  0.6368871 ], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([3.215], dtype=float32)>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(b'4.1812,52.0,5.701388888888889,0.9965277777777778,692.0,2.4027777777777777,33.73, -118.31,3.215')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3eb532b-3cb9-434e-9191-3337ec16376e",
   "metadata": {},
   "source": [
    "Looks good! We can now apply the function to the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00e0e07-1477-49ca-b88c-41f33703dbf4",
   "metadata": {},
   "source": [
    "### Putting EveryThing Together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fb9464-e5e2-40aa-901f-33fe77661da7",
   "metadata": {},
   "source": [
    "To make the code reusable, let's put together everything in small helper function: it will create and return a dataset that will efficiently load California housing dataset from multiple CSV files, preprocess it, shuffle it and optionally repeat it, and batch it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cdc1eae-ffdb-43b1-ac5f-3735e65b6d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_reader_dataset(filepaths, repeat=1, n_readers=5, n_read_threads=None,\n",
    "                       shuffle_buffer_size=10000, n_parse_threads=5, batch_size=32):\n",
    "    dataset = tf.data.Dataset.list_files(filepaths).repeat(repeat)\n",
    "    dataset = dataset.interleave(lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "                                cycle_length=n_readers, num_parallel_calls=n_read_threads)\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
    "    return dataset.batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d752e4-2548-4be3-8d21-c5d97a658038",
   "metadata": {},
   "source": [
    "The last line `prefetch(1)` is important for performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad211573-7b61-4e1d-a4ac-0ce534dd0ce1",
   "metadata": {},
   "source": [
    "### Prefetching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461354b4-9580-4822-92ab-d5968999005e",
   "metadata": {},
   "source": [
    "By calling `prefetch(1)` at the end, we are creating a dataset that will do its best to always be one batch ahead. (In general prefetching one batch is fine, but in some case we may need few more. We can always let tf decide on its own by passign `tf.data.AUTOTUNE`). In other words, while our training algorithm is working on one batch, the dataset will already be working in parallel on getting the next batch ready (e.g., reading the data from disk and preprocessing it). This can improve performace dramatically. If we also ensure that loading and preprocessing are multithreaded, we can exploit multiple cores on the CPU. \n",
    "\n",
    "If dataset is small enough to fit in memory, we can significantly speed up training by using the dataset's `cache()` method to cache its content to RAM. We should generally do this after loading and preprocessing data, but before shuffling, repeating, batching and prefetching. This way, each instance will only be read and preprocessed once (instead once per epoch), but the data will still be shuffled differently at each epoch, and the next batch will still be prepared in advance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc2f0d1-801b-4bdd-8d4d-be34efdbfd09",
   "metadata": {},
   "source": [
    "### Using the Dataset with tf.keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1eeb4c-9f78-4486-aa24-1cd3d73ddbd4",
   "metadata": {},
   "source": [
    "Now we have `csv_reader_dataset()` function to create a dataset for the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79d370ca-4c43-4284-ba8c-1f2ca4154346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = tf.Tensor(\n",
      "[[ 0.5804519  -0.20762321  0.05616303 -0.15191229  0.01343246  0.00604472\n",
      "   1.2525111  -1.3671792 ]\n",
      " [ 5.818099    1.8491895   1.1784915   0.28173092 -1.2496178  -0.3571987\n",
      "   0.7231292  -1.0023477 ]\n",
      " [-0.9253566   0.5834586  -0.7807257  -0.28213993 -0.36530012  0.27389365\n",
      "  -0.76194876  0.72684526]], shape=(3, 8), dtype=float32)\n",
      "y = tf.Tensor(\n",
      "[[1.752]\n",
      " [1.313]\n",
      " [1.535]], shape=(3, 1), dtype=float32)\n",
      "\n",
      "X = tf.Tensor(\n",
      "[[-0.8324941   0.6625668  -0.20741376 -0.18699841 -0.14536144  0.09635526\n",
      "   0.9807942  -0.67250353]\n",
      " [-0.62183803  0.5834586  -0.19862501 -0.3500319  -1.1437552  -0.3363751\n",
      "   1.107282   -0.8674123 ]\n",
      " [ 0.8683102   0.02970133  0.3427381  -0.29872298  0.7124906   0.28026953\n",
      "  -0.72915536  0.86178064]], shape=(3, 8), dtype=float32)\n",
      "y = tf.Tensor(\n",
      "[[0.919]\n",
      " [1.028]\n",
      " [2.182]], shape=(3, 1), dtype=float32)\n",
      "\n",
      "X = tf.Tensor(\n",
      "[[-1.0344558   1.0581076  -0.8869343  -0.08743899  0.6157541   0.4368748\n",
      "  -0.75726473  0.64688075]\n",
      " [ 0.4675818   0.6625668   0.03120424 -0.02621728 -0.8498953  -0.11217064\n",
      "  -0.860329    0.72684526]\n",
      " [-0.75702035  0.26702586 -0.60196173 -0.07209105  0.28082678  0.36254692\n",
      "  -0.7338394   0.7768212 ]], shape=(3, 8), dtype=float32)\n",
      "y = tf.Tensor(\n",
      "[[1.364]\n",
      " [2.314]\n",
      " [1.696]], shape=(3, 1), dtype=float32)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "train_set = csv_reader_dataset(train_filepaths, batch_size=3)\n",
    "for X_batch, y_batch in train_set.take(3):\n",
    "    print(\"X =\", X_batch)\n",
    "    print(\"y =\", y_batch)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29f71a25-874f-4a77-b416-33b327ad0189",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = csv_reader_dataset(train_filepaths, repeat=None)\n",
    "valid_set = csv_reader_dataset(valid_filepaths)\n",
    "test_set = csv_reader_dataset(test_filepaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bad5f17-71de-4ebb-92c8-31c359e54d22",
   "metadata": {},
   "source": [
    "And now we can simply build and train a Keras model using these datasets (support is specific for `tf.keras`). All we need to do is pass the training and validation datasets to the `fit()` method, instead of `X_train, y_train, X_vaild` and `y_valid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a29dd009-3dcf-4794-ba62-a4a605e2e8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21229f57-11db-4b71-80ef-3e77dc3bfb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b14cc65a-96aa-4da3-96e0-c5cba0a56c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "362/362 [==============================] - 1s 3ms/step - loss: 2.0315 - val_loss: 3.7452\n",
      "Epoch 2/10\n",
      "362/362 [==============================] - 1s 3ms/step - loss: 0.9367 - val_loss: 0.7827\n",
      "Epoch 3/10\n",
      "362/362 [==============================] - 1s 3ms/step - loss: 0.7497 - val_loss: 0.8117\n",
      "Epoch 4/10\n",
      "362/362 [==============================] - 1s 4ms/step - loss: 0.6687 - val_loss: 0.6846\n",
      "Epoch 5/10\n",
      "362/362 [==============================] - 1s 2ms/step - loss: 0.6179 - val_loss: 0.6467\n",
      "Epoch 6/10\n",
      "362/362 [==============================] - 1s 3ms/step - loss: 0.6187 - val_loss: 0.6151\n",
      "Epoch 7/10\n",
      "362/362 [==============================] - 1s 4ms/step - loss: 0.5609 - val_loss: 0.7972\n",
      "Epoch 8/10\n",
      "362/362 [==============================] - 1s 2ms/step - loss: 0.5372 - val_loss: 0.5258\n",
      "Epoch 9/10\n",
      "362/362 [==============================] - 1s 4ms/step - loss: 0.5180 - val_loss: 0.5510\n",
      "Epoch 10/10\n",
      "362/362 [==============================] - 1s 4ms/step - loss: 0.5200 - val_loss: 0.4858\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f5f045935d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=32\n",
    "model.fit(train_set, steps_per_epoch=len(X_train)//batch_size, epochs=10,\n",
    "         validation_data=valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6f3d60a-e524-4973-9bcb-1871e7fa2be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4921\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.49213576316833496"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_set, steps=len(X_test) // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c559513-9fcc-4ca9-aa7e-1d8856c745ba",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 8ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.7322209 ],\n",
       "       [1.3085666 ],\n",
       "       [1.5255079 ],\n",
       "       [1.0248382 ],\n",
       "       [2.5808408 ],\n",
       "       [2.2871757 ],\n",
       "       [1.7916274 ],\n",
       "       [4.3145485 ],\n",
       "       [2.206674  ],\n",
       "       [2.4540796 ],\n",
       "       [2.491219  ],\n",
       "       [2.857857  ],\n",
       "       [2.7704601 ],\n",
       "       [2.5426204 ],\n",
       "       [0.67148834],\n",
       "       [2.1122987 ],\n",
       "       [3.4075441 ],\n",
       "       [2.0837922 ],\n",
       "       [5.348416  ],\n",
       "       [1.9640238 ],\n",
       "       [3.9412053 ],\n",
       "       [1.339951  ],\n",
       "       [2.236424  ],\n",
       "       [1.6961956 ],\n",
       "       [2.6966777 ],\n",
       "       [1.5503691 ],\n",
       "       [1.9002333 ],\n",
       "       [2.3937144 ],\n",
       "       [3.0539567 ],\n",
       "       [1.2766786 ],\n",
       "       [2.2559    ],\n",
       "       [1.3738415 ],\n",
       "       [2.22063   ],\n",
       "       [0.91933674],\n",
       "       [5.45888   ],\n",
       "       [2.0175803 ],\n",
       "       [1.8049254 ],\n",
       "       [2.058518  ],\n",
       "       [1.6453042 ],\n",
       "       [3.0500243 ],\n",
       "       [1.1761464 ],\n",
       "       [1.2705905 ],\n",
       "       [4.044486  ],\n",
       "       [2.1326566 ],\n",
       "       [1.2013569 ],\n",
       "       [3.055455  ],\n",
       "       [2.1411066 ],\n",
       "       [1.0861909 ],\n",
       "       [1.3554629 ],\n",
       "       [1.2244564 ],\n",
       "       [2.0859795 ],\n",
       "       [2.509807  ],\n",
       "       [2.53593   ],\n",
       "       [1.5357313 ],\n",
       "       [2.328605  ],\n",
       "       [1.1619896 ],\n",
       "       [2.4998362 ],\n",
       "       [1.2269835 ],\n",
       "       [1.3960061 ],\n",
       "       [4.6563287 ],\n",
       "       [4.258176  ],\n",
       "       [3.5505226 ],\n",
       "       [2.5182273 ],\n",
       "       [1.7964044 ],\n",
       "       [2.6224167 ],\n",
       "       [1.0477208 ],\n",
       "       [1.4842416 ],\n",
       "       [3.1475916 ],\n",
       "       [0.9972979 ],\n",
       "       [2.1971037 ],\n",
       "       [1.902704  ],\n",
       "       [1.6066481 ],\n",
       "       [1.8536577 ],\n",
       "       [2.5204687 ],\n",
       "       [1.2242396 ],\n",
       "       [1.720289  ],\n",
       "       [0.9404229 ],\n",
       "       [1.5397108 ],\n",
       "       [1.8173523 ],\n",
       "       [3.2756157 ],\n",
       "       [3.7198763 ],\n",
       "       [2.4297867 ],\n",
       "       [2.2749891 ],\n",
       "       [1.7196877 ],\n",
       "       [2.6116598 ],\n",
       "       [1.0306777 ],\n",
       "       [2.2150636 ],\n",
       "       [2.1341648 ],\n",
       "       [2.560404  ],\n",
       "       [3.1973665 ],\n",
       "       [5.440608  ],\n",
       "       [2.1385133 ],\n",
       "       [2.9516919 ],\n",
       "       [3.304295  ],\n",
       "       [4.067764  ],\n",
       "       [1.7898543 ]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_set = test_set.take(3).map(lambda X, y: X) # pretend we have 3 new instances\n",
    "model.predict(new_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701cc04f-8bbd-4193-a447-206f95d1872c",
   "metadata": {},
   "source": [
    "If we want to build our own custom training loop, we can just iterate over the training set, very naturally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "afd27753-6a51-4849-b5b5-c32fa928f573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global step 1810/1810"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Nadam(learning_rate=0.01)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "\n",
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps_per_epoch = len(X_train) // batch_size\n",
    "total_steps = n_epochs * n_steps_per_epoch\n",
    "global_step = 0\n",
    "\n",
    "for X_batch, y_batch in train_set.take(total_steps):\n",
    "    global_step += 1\n",
    "    print(f\"\\rGlobal step {global_step}/{total_steps}\", end=\"\")\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X_batch)\n",
    "        main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "        loss = tf.add_n([main_loss] + model.losses)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c03774-3f14-44d0-855e-14a928e581a0",
   "metadata": {},
   "source": [
    "In fact, it is even possible to create TF Function, that performs the whole training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f0928a6-26af-4db7-9682-cb41f9a63c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=0.01)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "\n",
    "@tf.function\n",
    "def train(model, n_epochs, batch_size=32, n_readers=5, n_read_threads=5,\n",
    "         shuffle_buffer_size = 10000, n_parse_threads=5):\n",
    "    train_set = csv_reader_dataset(train_filepaths, repeat=n_epochs, n_readers=n_readers,\n",
    "                                  n_read_threads=n_read_threads,\n",
    "                                  shuffle_buffer_size=shuffle_buffer_size,\n",
    "                                  n_parse_threads=n_parse_threads, batch_size=batch_size)\n",
    "    for X_batch, y_batch in train_set:\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0063c3a9-6264-4af9-a002-b713047ea25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56287a76-c1e1-4f62-9543-e5b9b48da894",
   "metadata": {},
   "source": [
    "## The TFRecord Format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703a4ac9-4e39-48df-8bcb-c77b1bab0f9e",
   "metadata": {},
   "source": [
    "We can easily create a TFRecord file using the `tf.io.TFRecordWriter` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e074e156-6f59-4193-8ed8-bc0831254cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.io.TFRecordWriter(\"my_data.tfrecord\") as f:\n",
    "    f.write(b\"This is first record\")\n",
    "    f.write(b\"And this is second record\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b47e74-5af1-4a2a-9841-997d63a12784",
   "metadata": {},
   "source": [
    "And we can then use a `tf.data.TFRecordDataset` to read one or more TFRecord files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ea480681-f158-4bdc-a2f9-ac47763a7f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'This is first record', shape=(), dtype=string)\n",
      "tf.Tensor(b'And this is second record', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "filepaths = [\"my_data.tfrecord\"]\n",
    "dataset = tf.data.TFRecordDataset(filepaths)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd61bb4-b712-49e5-9234-235208920fb3",
   "metadata": {},
   "source": [
    "**TIP:**\n",
    "\n",
    "By default, a `TFRecordDataset` will read files one by one, but we can make it read multiple files in parallel and interleave their records by setting `num_parallel_reads`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (latest)",
   "language": "python",
   "name": "latest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
