{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6d7733e-1d62-47a2-94be-c97129d44625",
   "metadata": {},
   "source": [
    "# Custom Models and Training with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc661171-782a-4de4-8365-ef523a5c10f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e3ecb7-16da-43ae-b8f2-2163a77ad4d1",
   "metadata": {},
   "source": [
    "## Using TensorFlow like Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5509d0b0-9b89-448e-892d-4ae8b0ca5d41",
   "metadata": {},
   "source": [
    "TensorFlow's API revolves around *tensors*, which flow from operation to operation, hence the name Tensor*flow*. A tensor is usually a multidimensional array (exactly like Numpy `ndarray`), but it can also hold a scalar (a simple, value such as 42).\n",
    "\n",
    "These tensors will be important when we create custom cost functions, custom metrics, custom layers , and more. Let's see how to create and manipulate them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaf6667-8ea2-4bdd-b8da-1138562191fd",
   "metadata": {},
   "source": [
    "### Tensor and Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29922ad-1cca-4b8c-8760-fc6cb6367809",
   "metadata": {},
   "source": [
    "We can create a tensor with `tf.constant()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a27f630b-d45b-479d-a58a-33145f446ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tf.constant([[1., 2., 3.], [4., 5., 6.]]) # Here's a tensor representing matrix with two rows and three columns of floats\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb68ddd3-c0e6-48d2-bbba-c7ccaaa14ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=42>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(42) #scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1ca170-3fb8-42fc-8609-b1b753319ba8",
   "metadata": {},
   "source": [
    "Just like `ndarray`, a `tf.Tensor` has a shape and a datatype (dtype):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a8131c1-b39f-4bbe-8882-9268015c8624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b414dc4-0af0-48df-b37d-e459be81bbb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.float32"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21235964-6f3d-4905-9337-76bea4c2fe49",
   "metadata": {},
   "source": [
    "Indexing works much like NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad38bdbb-aeaa-40ac-b70e-981e1bb66984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[2., 3.],\n",
       "       [5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e295d6bb-590c-44e7-871f-7250031121d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
       "array([[2.],\n",
       "       [5.]], dtype=float32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[..., 1, tf.newaxis] # ... is same as :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "992a0c56-d9fa-4988-a536-74009a210f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
       "array([[2.],\n",
       "       [5.]], dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[:, 1, tf.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78f99d9-8d9a-4671-94b5-8c369595c2e4",
   "metadata": {},
   "source": [
    "Most importantly, all sorts of tensor operations are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1bd30c9-2d6d-4127-98cf-d0bdc0338dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[11., 12., 13.],\n",
       "       [14., 15., 16.]], dtype=float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "abbfa763-e2ed-4381-9d69-4e305aa5b079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[ 1.,  4.,  9.],\n",
       "       [16., 25., 36.]], dtype=float32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.square(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c39d033-a591-4f14-88df-397b54c55d28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[14., 32.],\n",
       "       [32., 77.]], dtype=float32)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t @ tf.transpose(t) # @ is for multiplication, equivalent to tf.matmul()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06670d0-8887-4b30-aa38-f107cd390240",
   "metadata": {},
   "source": [
    "Note that writing `t + 10` is equivalent to calling `tf.add(t, 10)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ac31c2-ebe3-4f62-9e1b-ee508c929c40",
   "metadata": {},
   "source": [
    "### Tensor and Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e623753-7373-4fe7-9f37-e39843aa6302",
   "metadata": {},
   "source": [
    "Tensors play nice with Numpy: we can create a tensor from Numpy array and vice versa. We can even apply TensorFlow operations to NumPy arrays and NumPy operations to tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52ec891b-3ea3-42cc-b1b4-e0be42c154b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 4., 5.])>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([2., 4., 5.])\n",
    "tf.constant(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7e8fc57-0ec0-4481-b591-7c6a63291f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.numpy() # or np.array(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25817c6d-2ee9-4f07-b29a-b6b5d2027cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float64, numpy=array([ 4., 16., 25.])>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.square(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2ef5e306-a612-4216-b6c9-d1e42f4f30be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  4.,  9.],\n",
       "       [16., 25., 36.]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.square(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d45ff5-10dc-4ab3-a3e8-9704f02d98b4",
   "metadata": {},
   "source": [
    "**WARNING:**\n",
    "\n",
    "Notice that Numpy uses a 64-bit precision by default, while TensorFlow uses 32-bit. This is because 32-bit precision is generally more than enough for neural networks, plus it runs faster and uses less RAM. So when we create a tensor from Numpy array, make sure to set `dtype=tf.float32`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e20e58-b119-4c49-b025-286bf2a1ad0d",
   "metadata": {},
   "source": [
    "### Type Conversions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fc714d-801e-4f06-98e5-74ec5363705c",
   "metadata": {},
   "source": [
    "Type conversions can significantly hurt performance, and they can easily gets unnoticed when they are done automatically. To avoid this, TensorFlow does not perform any type conversion automatically: it just raises an exception if we try to execute an operation on tensors with compatible types. For example: we cannot add a float tensor and an integer tensor, and we cannot even add a 32-bit float and a 64-bit float:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "afa7eeba-2950-4964-a804-b71eff2833cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a int32 tensor [Op:AddV2] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tf\u001b[38;5;241m.\u001b[39mconstant(\u001b[38;5;241m2.\u001b[39m) \u001b[38;5;241m+\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant(\u001b[38;5;241m40\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/latest/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/latest/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:6656\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6654\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   6655\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 6656\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a int32 tensor [Op:AddV2] name: "
     ]
    }
   ],
   "source": [
    "tf.constant(2.) + tf.constant(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9ff62a6c-9346-4834-afc5-b5f93a060079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a int32 tensor [Op:AddV2] name: \n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tf.constant(2.) + tf.constant(40)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "434f8bd2-0941-4998-be8a-d6c2e1f314d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a double tensor [Op:AddV2] name: \n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tf.constant(2.) + tf.constant(40., dtype=tf.float64)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203724d7-afc7-4dd6-b0a1-87fa729ac21d",
   "metadata": {},
   "source": [
    "This may be a bit annoying at first, but remember that it's for good cause! And ofcourse we can use `tf.cast()` when we really need to convert types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "61644c9b-fbfa-451f-89c4-a82d6b4eb4fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=42.0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = tf.constant(40, dtype=tf.float64)\n",
    "tf.constant(2.0) + tf.cast(t2, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75252a75-a74d-49af-822e-a5143e3932ee",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a6ab8d-04c3-4257-8033-d168508a97fb",
   "metadata": {},
   "source": [
    "The `tf.Tensor` values we've seen so far are immutables: we cannot modify them. This means that we cannot use regular tensors to implement weights in neural networks, since they needs to be tweaked by backpropogation. What we need is `tf.Variable`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b8a5e4ea-d114-4671-ae02-af742ddaa00c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2, 3) dtype=int32, numpy=\n",
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]], dtype=int32)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = tf.Variable([[1, 2, 3], [4, 5, 6]])\n",
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d4dee4-cec5-4dc1-905d-452f48f73519",
   "metadata": {},
   "source": [
    "A `tf.Variable` acts much like a `tf.Tensor`: we can perform the same operations with it, it plays nicely with Numpy as well, and it is just as picky with types. But it can also be modified in place using `assign()` method (direct assignment will not work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1a83a2b3-f123-4116-be35-d5723dd75eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2, 3) dtype=int32, numpy=\n",
       "array([[ 2,  4,  6],\n",
       "       [ 8, 10, 12]], dtype=int32)>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.assign(2*v)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dc48dccb-e7c5-4300-b64b-9758da83b0f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=int32, numpy=\n",
       "array([[ 2, 42,  6],\n",
       "       [ 8, 10, 12]], dtype=int32)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[0,1].assign(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1a51303d-5133-4d13-9b0c-01e52140ac7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=int32, numpy=\n",
       "array([[ 2, 42,  0],\n",
       "       [ 8, 10,  1]], dtype=int32)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[:, 2].assign([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "65ae1315-b355-435c-85d1-127e09dba302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=int32, numpy=\n",
       "array([[100,  42,   0],\n",
       "       [  8,  10, 200]], dtype=int32)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.scatter_nd_update(indices=[[0,0], [1,2]], updates=[100, 200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b821ba52-257a-44b0-a543-6f2c3834241d",
   "metadata": {},
   "source": [
    "**NOTE:**\n",
    "\n",
    "In practice we will rarely have to create variables manually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11ec91d-7f8b-4b82-8b43-466e42ef7dfa",
   "metadata": {},
   "source": [
    "### Other Data Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e291bb-6ab7-49e9-ae45-24a77d2c7baf",
   "metadata": {},
   "source": [
    "- Sparse tensors (`tf.SparseTensor`) => Tensors containing mostly zeros\n",
    "- Tensor Arrays (`tf.TensorArray`) => List of tensors\n",
    "- Ragged Tensors (`tf.RaggedTensor`) => static list of lists of tensors. Every tensor has same shape and data type\n",
    "- String Tensors => Regular tensor of type `tf.string`\n",
    "- Sets\n",
    "- Queues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c2669a-325b-4fec-8178-b0a8670f5c90",
   "metadata": {},
   "source": [
    "## Customizing Model and Training Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f869e06e-e792-428e-8131-22fb8075ec70",
   "metadata": {},
   "source": [
    "Let's start by creating a custom loss function, which is simple and common use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a41765-ab61-4503-a8b2-0c0a6d3ecea3",
   "metadata": {},
   "source": [
    "Let's start by loading and preparing the California housing dataset. We first load it, then split it into a training set, a validation set and a test set, and finally we scale it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "392ecb46-e772-4cae-8c74-02e27276ddef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f62235-2056-4e62-956c-3445dc39fcc6",
   "metadata": {},
   "source": [
    "### Custom Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb6ff31-88b6-4e4a-94e6-cf2ae3702990",
   "metadata": {},
   "source": [
    "Suppose we want to train a linear regression model, but our training set is a bit noisy. Ofcourse, we start by trying to clean up the dataset by removing or fixing the outliers, but that turns out to be insufficient; the dataset is still noisy. \n",
    "\n",
    "Which loss function should we use? The mean squared error might penalize large errors too much and cause model to be imprecise. The mean absolute error would not penalize outliers as much, but training might take a while to converge, and trained model not be very precise. This is probably good time to use Huber loss instead of the good old MSE. \n",
    "\n",
    "The Huber loss is not currently part of the official Keras API, but it is available in tf.keras (just use an instance of the `keras.losses.Huber` class). But let's pretend it's not there: implementing it is easy as pie!.\n",
    "\n",
    "Just create a function that takes the labels and parameters as arguments, and use TensorFlow operations to compute every instance's loss:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b7c548-2b0b-4c7a-8b7e-46c33dfbd253",
   "metadata": {},
   "source": [
    "Hubber Loss:\n",
    "$$\n",
    "L_{\\delta}(a) =\n",
    "\\begin{cases}\n",
    "    \\frac{1}{2} a^2 & \\text{for |a|} \\le \\delta \\\\\n",
    "    \\delta \\cdot (|a| - \\frac{1}{2} \\cdot \\delta) , & otherwise\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "354c0c5d-28af-424e-b99d-eb865830efd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_loss_fn(y_true, y_pred):\n",
    "    error = y_true - y_pred\n",
    "    is_small_error = tf.abs(error) < 1 # here the delta = 1\n",
    "    squared_loss = tf.square(error) / 2\n",
    "    linear_loss = 1 * (tf.abs(error) - (0.5 * 1))\n",
    "    return tf.where(is_small_error, squared_loss, linear_loss) # returns squared loss when error is small, else returns linear loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd0e5eb-bbaf-4431-86cb-62a5526345e2",
   "metadata": {},
   "source": [
    "It is also preferable to return a tensor containing one loss per instance, rather than returning the mean loss. This way, Keras can apply class weights or sample weights when requested."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f801980-abc8-4184-a249-3daeefe6620e",
   "metadata": {},
   "source": [
    "Now we can use this loss when we compile the Keras model, then train our model. When compiling the model, just pass this function to `loss` argument like this:\n",
    "```\n",
    "model.compile(loss=huber_loss_fn, optimizer=\"nadam\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1b8b150b-bd44-4343-bc78-5e80b0a81529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 3s 5ms/step - loss: 0.5731 - mae: 0.9348 - val_loss: 0.3381 - val_mae: 0.6284\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.2123 - mae: 0.5053 - val_loss: 0.2907 - val_mae: 0.5726\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f90e4c42090>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = X_train.shape[1:] # (8,1)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                      input_shape=input_shape),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=huber_loss_fn, optimizer=\"nadam\", metrics=[\"mae\"])\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08252c3-1f63-4a5d-b896-a884efca25ca",
   "metadata": {},
   "source": [
    "And that's it! For each batch during training, Keras will call the `huber_fn()` to compute the loss and use it to perform a Gradient Descent step. Moreover, it will keep track of the total loss since the beginning of the epoch, and it will display the mean loss. \n",
    "\n",
    "But what happens to this custom loss when we save the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11461e44-c527-4369-9d68-4379c51fa841",
   "metadata": {},
   "source": [
    "### Saving and Loading Models That Contain Custom Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b1dc2c02-3720-4a9a-8c76-209c1eee0ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_model_with_a_custom_loss.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e05d26-2703-4d6a-88aa-638675aef256",
   "metadata": {},
   "source": [
    "Saving a model containing custom loss function works fine, as Keras saves the name of the function. Whenever we load it, we we'll need to provide a dictionary that maps the function name to the actual function.\n",
    "\n",
    "More generally, when we load a model containing custom objects, we need to map the names of the objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8ea4bb4a-fdd6-43e1-80a0-3f1353f737a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"my_model_with_a_custom_loss.keras\", \n",
    "                               custom_objects={\"huber_loss_fn\": huber_loss_fn})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e25377ac-2da3-49be-96f1-cb6491b5c31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.2017 - mae: 0.4893 - val_loss: 0.2442 - val_mae: 0.5253\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.1952 - mae: 0.4809 - val_loss: 0.2016 - val_mae: 0.4813\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f90e49d1890>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9784c7-3497-4500-8450-c5759a6938f1",
   "metadata": {},
   "source": [
    "With the current implementation, any error between -1 and 1 is considered \"small\". But what if we want a different threshold? One solution is to create a function that creates a configured loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "368d2005-b9c4-4c2b-999c-2caf10e4fd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_huber(threshold=1.0):\n",
    "    def huber_fn(y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < threshold \n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = threshold * tf.abs(error) - threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    return huber_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f16b4ee9-585f-42cc-abb5-47ebe05a3d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=create_huber(2.0), optimizer=\"nadam\", metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3504ad52-2c0d-451e-96bb-223aa50b8b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2145 - mae: 0.4774 - val_loss: 0.2259 - val_mae: 0.4760\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2099 - mae: 0.4727 - val_loss: 0.2013 - val_mae: 0.4587\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f9104e0efd0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "74ac1112-6823-4c59-ae61-5c4e648131c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_model_with_custom_loss_threshold_2.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1a6ea2-5d5b-4a2e-bd3f-08ab601f009c",
   "metadata": {},
   "source": [
    "Unfortunately, when we save the model, the `threshold` will not be saved. This means that we will have to specify the `threshold` value when loading the model (note that the name to use is \"`huber_fn`\", which is the name of the function we gave Keras, not the name of the function that created it):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4cff4ab6-dca3-468a-82aa-1b3533101688",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"my_model_with_custom_loss_threshold_2.keras\",\n",
    "                               custom_objects={\"huber_fn\": create_huber(2.0)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "984399e6-8af9-453a-86c1-29a6b2d7fed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 2s 2ms/step - loss: 0.2052 - mae: 0.4665 - val_loss: 0.2369 - val_mae: 0.4675\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2043 - mae: 0.4639 - val_loss: 0.2005 - val_mae: 0.4569\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f9104ce6fd0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e846abc-3dc6-4fdd-9264-6976081e0c23",
   "metadata": {},
   "source": [
    "We can solve this by creating a subclass of `keras.losses.Loss` class, and then implementing its `get_config()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "00e5b199-791f-41a2-98fa-59bbad978757",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberLoss(keras.losses.Loss):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        self.threshold = threshold\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < self.threshold \n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = self.threshold * tf.abs(error) - self.threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af2a3e2-3a84-4455-9022-5e5b8524535e",
   "metadata": {},
   "source": [
    "Explanation of above code:\n",
    "\n",
    "- The constructor accepts ``**kwargs` and passes them to the parent constructor, which handles standard hyperparameters: the `name` of the loss and the `reduction` algorithm to use to aggregate the individual instance losses. By default, it is ``\"sum_over_batch_size\"`, which means that the loss will be the sum of the instance losses, weighted by the sample weights, if any, and divided by the batch size (not by the sum of weights, so this is not the weighted mean). Other possible values are `\"sum\"` and `None`.\n",
    "\n",
    "- The `call()` method takes the labels and predictions, computes all the instance losses, and returns them.\n",
    "\n",
    "- The `get_config()` method returns a dictionary mapping each hyperparameter name to its value. It first calls the parent class’s `get_config()` method, then adds the new hyperparameters to this dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9105008-0de1-4900-8dc5-27f4ec976cc9",
   "metadata": {},
   "source": [
    "We can than use any instance of the class when we compile the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1d778339-c07d-49b6-935b-2864b29cdb55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 2s 2ms/step - loss: 0.9749 - mae: 1.0742 - val_loss: 0.4231 - val_mae: 0.6038\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2504 - mae: 0.5184 - val_loss: 0.3407 - val_mae: 0.5439\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       input_shape=input_shape),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "model.compile(loss=HuberLoss(2.0), optimizer=\"nadam\", metrics=[\"mae\"])\n",
    "\n",
    "h = model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fae8044-8153-44c7-9e20-f60124559325",
   "metadata": {},
   "source": [
    "When we save the model, the threshold will get saved along with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "657a029e-4abf-4aff-81a1-a39107acdbb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model_with_custom_loss_class.kears/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model_with_custom_loss_class.kears/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"my_model_with_custom_loss_class.kears\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b197d884-88b6-4897-98d6-d569bbd4a54b",
   "metadata": {},
   "source": [
    "And when we load the model, we just need to map the class name to class itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3dd1c95e-7cac-4691-b546-b38b36d19a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"my_model_with_custom_loss_class.keras\",\n",
    "                               custom_objects={\"HuberLoss\": HuberLoss})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd06ba15-a708-4617-99ca-2867a0ddfbba",
   "metadata": {},
   "source": [
    "When we save a model, Keras calls the loss instance's `get_config()` method and saves the config as the JSON. When we load the model, it calls the `from_config()` class method on `HuberLoss` class: this method is implemented by the base class (Loss) and creates an instance of the class, passing `**config` to the constructor. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d0423b-7983-41c8-a3b8-52126e5b14f8",
   "metadata": {},
   "source": [
    "That's it for losses! Similar for activation functions, initializers, regularizers and constraints. Let's look at these now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64c0242-be57-4ace-9ceb-4806ae51990a",
   "metadata": {},
   "source": [
    "### Custom Activation Functions, Initializers, Regularizers and Constraints"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (latest)",
   "language": "python",
   "name": "latest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
