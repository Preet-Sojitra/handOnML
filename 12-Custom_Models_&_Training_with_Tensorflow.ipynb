{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6d7733e-1d62-47a2-94be-c97129d44625",
   "metadata": {},
   "source": [
    "# Custom Models and Training with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc661171-782a-4de4-8365-ef523a5c10f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e3ecb7-16da-43ae-b8f2-2163a77ad4d1",
   "metadata": {},
   "source": [
    "## Using TensorFlow like Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5509d0b0-9b89-448e-892d-4ae8b0ca5d41",
   "metadata": {},
   "source": [
    "TensorFlow's API revolves around *tensors*, which flow from operation to operation, hence the name Tensor*flow*. A tensor is usually a multidimensional array (exactly like Numpy `ndarray`), but it can also hold a scalar (a simple, value such as 42).\n",
    "\n",
    "These tensors will be important when we create custom cost functions, custom metrics, custom layers , and more. Let's see how to create and manipulate them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaf6667-8ea2-4bdd-b8da-1138562191fd",
   "metadata": {},
   "source": [
    "### Tensor and Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29922ad-1cca-4b8c-8760-fc6cb6367809",
   "metadata": {},
   "source": [
    "We can create a tensor with `tf.constant()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a27f630b-d45b-479d-a58a-33145f446ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tf.constant([[1., 2., 3.], [4., 5., 6.]]) # Here's a tensor representing matrix with two rows and three columns of floats\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb68ddd3-c0e6-48d2-bbba-c7ccaaa14ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=42>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(42) #scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1ca170-3fb8-42fc-8609-b1b753319ba8",
   "metadata": {},
   "source": [
    "Just like `ndarray`, a `tf.Tensor` has a shape and a datatype (dtype):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a8131c1-b39f-4bbe-8882-9268015c8624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b414dc4-0af0-48df-b37d-e459be81bbb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.float32"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21235964-6f3d-4905-9337-76bea4c2fe49",
   "metadata": {},
   "source": [
    "Indexing works much like NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad38bdbb-aeaa-40ac-b70e-981e1bb66984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[2., 3.],\n",
       "       [5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e295d6bb-590c-44e7-871f-7250031121d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
       "array([[2.],\n",
       "       [5.]], dtype=float32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[..., 1, tf.newaxis] # ... is same as :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "992a0c56-d9fa-4988-a536-74009a210f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
       "array([[2.],\n",
       "       [5.]], dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[:, 1, tf.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78f99d9-8d9a-4671-94b5-8c369595c2e4",
   "metadata": {},
   "source": [
    "Most importantly, all sorts of tensor operations are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1bd30c9-2d6d-4127-98cf-d0bdc0338dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[11., 12., 13.],\n",
       "       [14., 15., 16.]], dtype=float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "abbfa763-e2ed-4381-9d69-4e305aa5b079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[ 1.,  4.,  9.],\n",
       "       [16., 25., 36.]], dtype=float32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.square(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c39d033-a591-4f14-88df-397b54c55d28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[14., 32.],\n",
       "       [32., 77.]], dtype=float32)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t @ tf.transpose(t) # @ is for multiplication, equivalent to tf.matmul()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06670d0-8887-4b30-aa38-f107cd390240",
   "metadata": {},
   "source": [
    "Note that writing `t + 10` is equivalent to calling `tf.add(t, 10)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ac31c2-ebe3-4f62-9e1b-ee508c929c40",
   "metadata": {},
   "source": [
    "### Tensor and Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e623753-7373-4fe7-9f37-e39843aa6302",
   "metadata": {},
   "source": [
    "Tensors play nice with Numpy: we can create a tensor from Numpy array and vice versa. We can even apply TensorFlow operations to NumPy arrays and NumPy operations to tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52ec891b-3ea3-42cc-b1b4-e0be42c154b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 4., 5.])>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([2., 4., 5.])\n",
    "tf.constant(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7e8fc57-0ec0-4481-b591-7c6a63291f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.numpy() # or np.array(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25817c6d-2ee9-4f07-b29a-b6b5d2027cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float64, numpy=array([ 4., 16., 25.])>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.square(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2ef5e306-a612-4216-b6c9-d1e42f4f30be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  4.,  9.],\n",
       "       [16., 25., 36.]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.square(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d45ff5-10dc-4ab3-a3e8-9704f02d98b4",
   "metadata": {},
   "source": [
    "**WARNING:**\n",
    "\n",
    "Notice that Numpy uses a 64-bit precision by default, while TensorFlow uses 32-bit. This is because 32-bit precision is generally more than enough for neural networks, plus it runs faster and uses less RAM. So when we create a tensor from Numpy array, make sure to set `dtype=tf.float32`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e20e58-b119-4c49-b025-286bf2a1ad0d",
   "metadata": {},
   "source": [
    "### Type Conversions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fc714d-801e-4f06-98e5-74ec5363705c",
   "metadata": {},
   "source": [
    "Type conversions can significantly hurt performance, and they can easily gets unnoticed when they are done automatically. To avoid this, TensorFlow does not perform any type conversion automatically: it just raises an exception if we try to execute an operation on tensors with compatible types. For example: we cannot add a float tensor and an integer tensor, and we cannot even add a 32-bit float and a 64-bit float:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "afa7eeba-2950-4964-a804-b71eff2833cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a int32 tensor [Op:AddV2] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tf\u001b[38;5;241m.\u001b[39mconstant(\u001b[38;5;241m2.\u001b[39m) \u001b[38;5;241m+\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant(\u001b[38;5;241m40\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/latest/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/latest/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:6656\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6654\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   6655\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 6656\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a int32 tensor [Op:AddV2] name: "
     ]
    }
   ],
   "source": [
    "tf.constant(2.) + tf.constant(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9ff62a6c-9346-4834-afc5-b5f93a060079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a int32 tensor [Op:AddV2] name: \n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tf.constant(2.) + tf.constant(40)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "434f8bd2-0941-4998-be8a-d6c2e1f314d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a double tensor [Op:AddV2] name: \n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tf.constant(2.) + tf.constant(40., dtype=tf.float64)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203724d7-afc7-4dd6-b0a1-87fa729ac21d",
   "metadata": {},
   "source": [
    "This may be a bit annoying at first, but remember that it's for good cause! And ofcourse we can use `tf.cast()` when we really need to convert types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "61644c9b-fbfa-451f-89c4-a82d6b4eb4fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=42.0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = tf.constant(40, dtype=tf.float64)\n",
    "tf.constant(2.0) + tf.cast(t2, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75252a75-a74d-49af-822e-a5143e3932ee",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a6ab8d-04c3-4257-8033-d168508a97fb",
   "metadata": {},
   "source": [
    "The `tf.Tensor` values we've seen so far are immutables: we cannot modify them. This means that we cannot use regular tensors to implement weights in neural networks, since they needs to be tweaked by backpropogation. What we need is `tf.Variable`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b8a5e4ea-d114-4671-ae02-af742ddaa00c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2, 3) dtype=int32, numpy=\n",
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]], dtype=int32)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = tf.Variable([[1, 2, 3], [4, 5, 6]])\n",
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d4dee4-cec5-4dc1-905d-452f48f73519",
   "metadata": {},
   "source": [
    "A `tf.Variable` acts much like a `tf.Tensor`: we can perform the same operations with it, it plays nicely with Numpy as well, and it is just as picky with types. But it can also be modified in place using `assign()` method (direct assignment will not work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1a83a2b3-f123-4116-be35-d5723dd75eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2, 3) dtype=int32, numpy=\n",
       "array([[ 2,  4,  6],\n",
       "       [ 8, 10, 12]], dtype=int32)>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.assign(2*v)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dc48dccb-e7c5-4300-b64b-9758da83b0f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=int32, numpy=\n",
       "array([[ 2, 42,  6],\n",
       "       [ 8, 10, 12]], dtype=int32)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[0,1].assign(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1a51303d-5133-4d13-9b0c-01e52140ac7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=int32, numpy=\n",
       "array([[ 2, 42,  0],\n",
       "       [ 8, 10,  1]], dtype=int32)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[:, 2].assign([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "65ae1315-b355-435c-85d1-127e09dba302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=int32, numpy=\n",
       "array([[100,  42,   0],\n",
       "       [  8,  10, 200]], dtype=int32)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.scatter_nd_update(indices=[[0,0], [1,2]], updates=[100, 200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b821ba52-257a-44b0-a543-6f2c3834241d",
   "metadata": {},
   "source": [
    "**NOTE:**\n",
    "\n",
    "In practice we will rarely have to create variables manually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11ec91d-7f8b-4b82-8b43-466e42ef7dfa",
   "metadata": {},
   "source": [
    "### Other Data Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e291bb-6ab7-49e9-ae45-24a77d2c7baf",
   "metadata": {},
   "source": [
    "- Sparse tensors (`tf.SparseTensor`) => Tensors containing mostly zeros\n",
    "- Tensor Arrays (`tf.TensorArray`) => List of tensors\n",
    "- Ragged Tensors (`tf.RaggedTensor`) => static list of lists of tensors. Every tensor has same shape and data type\n",
    "- String Tensors => Regular tensor of type `tf.string`\n",
    "- Sets\n",
    "- Queues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c2669a-325b-4fec-8178-b0a8670f5c90",
   "metadata": {},
   "source": [
    "## Customizing Model and Training Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f869e06e-e792-428e-8131-22fb8075ec70",
   "metadata": {},
   "source": [
    "Let's start by creating a custom loss function, which is simple and common use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a41765-ab61-4503-a8b2-0c0a6d3ecea3",
   "metadata": {},
   "source": [
    "Let's start by loading and preparing the California housing dataset. We first load it, then split it into a training set, a validation set and a test set, and finally we scale it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "392ecb46-e772-4cae-8c74-02e27276ddef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f62235-2056-4e62-956c-3445dc39fcc6",
   "metadata": {},
   "source": [
    "### Custom Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb6ff31-88b6-4e4a-94e6-cf2ae3702990",
   "metadata": {},
   "source": [
    "Suppose we want to train a linear regression model, but our training set is a bit noisy. Ofcourse, we start by trying to clean up the dataset by removing or fixing the outliers, but that turns out to be insufficient; the dataset is still noisy. \n",
    "\n",
    "Which loss function should we use? The mean squared error might penalize large errors too much and cause model to be imprecise. The mean absolute error would not penalize outliers as much, but training might take a while to converge, and trained model not be very precise. This is probably good time to use Huber loss instead of the good old MSE. \n",
    "\n",
    "The Huber loss is not currently part of the official Keras API, but it is available in tf.keras (just use an instance of the `keras.losses.Huber` class). But let's pretend it's not there: implementing it is easy as pie!.\n",
    "\n",
    "Just create a function that takes the labels and parameters as arguments, and use TensorFlow operations to compute every instance's loss:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b7c548-2b0b-4c7a-8b7e-46c33dfbd253",
   "metadata": {},
   "source": [
    "Hubber Loss:\n",
    "$$\n",
    "L_{\\delta}(a) =\n",
    "\\begin{cases}\n",
    "    \\frac{1}{2} a^2 & \\text{for |a|} \\le \\delta \\\\\n",
    "    \\delta \\cdot (|a| - \\frac{1}{2} \\cdot \\delta) , & otherwise\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "354c0c5d-28af-424e-b99d-eb865830efd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_loss_fn(y_true, y_pred):\n",
    "    error = y_true - y_pred\n",
    "    is_small_error = tf.abs(error) < 1 # here the delta = 1\n",
    "    squared_loss = tf.square(error) / 2\n",
    "    linear_loss = 1 * (tf.abs(error) - (0.5 * 1))\n",
    "    return tf.where(is_small_error, squared_loss, linear_loss) # returns squared loss when error is small, else returns linear loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd0e5eb-bbaf-4431-86cb-62a5526345e2",
   "metadata": {},
   "source": [
    "It is also preferable to return a tensor containing one loss per instance, rather than returning the mean loss. This way, Keras can apply class weights or sample weights when requested."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f801980-abc8-4184-a249-3daeefe6620e",
   "metadata": {},
   "source": [
    "Now we can use this loss when we compile the Keras model, then train our model. When compiling the model, just pass this function to `loss` argument like this:\n",
    "```\n",
    "model.compile(loss=huber_loss_fn, optimizer=\"nadam\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1b8b150b-bd44-4343-bc78-5e80b0a81529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 3s 5ms/step - loss: 0.5731 - mae: 0.9348 - val_loss: 0.3381 - val_mae: 0.6284\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.2123 - mae: 0.5053 - val_loss: 0.2907 - val_mae: 0.5726\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f90e4c42090>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = X_train.shape[1:] # (8,1)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                      input_shape=input_shape),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=huber_loss_fn, optimizer=\"nadam\", metrics=[\"mae\"])\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08252c3-1f63-4a5d-b896-a884efca25ca",
   "metadata": {},
   "source": [
    "And that's it! For each batch during training, Keras will call the `huber_fn()` to compute the loss and use it to perform a Gradient Descent step. Moreover, it will keep track of the total loss since the beginning of the epoch, and it will display the mean loss. \n",
    "\n",
    "But what happens to this custom loss when we save the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11461e44-c527-4369-9d68-4379c51fa841",
   "metadata": {},
   "source": [
    "### Saving and Loading Models That Contain Custom Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b1dc2c02-3720-4a9a-8c76-209c1eee0ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_model_with_a_custom_loss.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e05d26-2703-4d6a-88aa-638675aef256",
   "metadata": {},
   "source": [
    "Saving a model containing custom loss function works fine, as Keras saves the name of the function. Whenever we load it, we we'll need to provide a dictionary that maps the function name to the actual function.\n",
    "\n",
    "More generally, when we load a model containing custom objects, we need to map the names of the objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8ea4bb4a-fdd6-43e1-80a0-3f1353f737a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"my_model_with_a_custom_loss.keras\", \n",
    "                               custom_objects={\"huber_loss_fn\": huber_loss_fn})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e25377ac-2da3-49be-96f1-cb6491b5c31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.2017 - mae: 0.4893 - val_loss: 0.2442 - val_mae: 0.5253\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.1952 - mae: 0.4809 - val_loss: 0.2016 - val_mae: 0.4813\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f90e49d1890>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9784c7-3497-4500-8450-c5759a6938f1",
   "metadata": {},
   "source": [
    "With the current implementation, any error between -1 and 1 is considered \"small\". But what if we want a different threshold? One solution is to create a function that creates a configured loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "368d2005-b9c4-4c2b-999c-2caf10e4fd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_huber(threshold=1.0):\n",
    "    def huber_fn(y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < threshold \n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = threshold * tf.abs(error) - threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    return huber_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f16b4ee9-585f-42cc-abb5-47ebe05a3d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=create_huber(2.0), optimizer=\"nadam\", metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3504ad52-2c0d-451e-96bb-223aa50b8b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2145 - mae: 0.4774 - val_loss: 0.2259 - val_mae: 0.4760\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2099 - mae: 0.4727 - val_loss: 0.2013 - val_mae: 0.4587\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f9104e0efd0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "74ac1112-6823-4c59-ae61-5c4e648131c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_model_with_custom_loss_threshold_2.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1a6ea2-5d5b-4a2e-bd3f-08ab601f009c",
   "metadata": {},
   "source": [
    "Unfortunately, when we save the model, the `threshold` will not be saved. This means that we will have to specify the `threshold` value when loading the model (note that the name to use is \"`huber_fn`\", which is the name of the function we gave Keras, not the name of the function that created it):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4cff4ab6-dca3-468a-82aa-1b3533101688",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"my_model_with_custom_loss_threshold_2.keras\",\n",
    "                               custom_objects={\"huber_fn\": create_huber(2.0)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "984399e6-8af9-453a-86c1-29a6b2d7fed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 2s 2ms/step - loss: 0.2052 - mae: 0.4665 - val_loss: 0.2369 - val_mae: 0.4675\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2043 - mae: 0.4639 - val_loss: 0.2005 - val_mae: 0.4569\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f9104ce6fd0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e846abc-3dc6-4fdd-9264-6976081e0c23",
   "metadata": {},
   "source": [
    "We can solve this by creating a subclass of `keras.losses.Loss` class, and then implementing its `get_config()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "00e5b199-791f-41a2-98fa-59bbad978757",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberLoss(keras.losses.Loss):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        self.threshold = threshold\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < self.threshold \n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = self.threshold * tf.abs(error) - self.threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af2a3e2-3a84-4455-9022-5e5b8524535e",
   "metadata": {},
   "source": [
    "Explanation of above code:\n",
    "\n",
    "- The constructor accepts ``**kwargs` and passes them to the parent constructor, which handles standard hyperparameters: the `name` of the loss and the `reduction` algorithm to use to aggregate the individual instance losses. By default, it is ``\"sum_over_batch_size\"`, which means that the loss will be the sum of the instance losses, weighted by the sample weights, if any, and divided by the batch size (not by the sum of weights, so this is not the weighted mean). Other possible values are `\"sum\"` and `None`.\n",
    "\n",
    "- The `call()` method takes the labels and predictions, computes all the instance losses, and returns them.\n",
    "\n",
    "- The `get_config()` method returns a dictionary mapping each hyperparameter name to its value. It first calls the parent class’s `get_config()` method, then adds the new hyperparameters to this dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9105008-0de1-4900-8dc5-27f4ec976cc9",
   "metadata": {},
   "source": [
    "We can than use any instance of the class when we compile the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1d778339-c07d-49b6-935b-2864b29cdb55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 2s 2ms/step - loss: 0.9749 - mae: 1.0742 - val_loss: 0.4231 - val_mae: 0.6038\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2504 - mae: 0.5184 - val_loss: 0.3407 - val_mae: 0.5439\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       input_shape=input_shape),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "model.compile(loss=HuberLoss(2.0), optimizer=\"nadam\", metrics=[\"mae\"])\n",
    "\n",
    "h = model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fae8044-8153-44c7-9e20-f60124559325",
   "metadata": {},
   "source": [
    "When we save the model, the threshold will get saved along with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "657a029e-4abf-4aff-81a1-a39107acdbb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model_with_custom_loss_class.kears/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model_with_custom_loss_class.kears/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"my_model_with_custom_loss_class.kears\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b197d884-88b6-4897-98d6-d569bbd4a54b",
   "metadata": {},
   "source": [
    "And when we load the model, we just need to map the class name to class itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3dd1c95e-7cac-4691-b546-b38b36d19a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"my_model_with_custom_loss_class.keras\",\n",
    "                               custom_objects={\"HuberLoss\": HuberLoss})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd06ba15-a708-4617-99ca-2867a0ddfbba",
   "metadata": {},
   "source": [
    "When we save a model, Keras calls the loss instance's `get_config()` method and saves the config as the JSON. When we load the model, it calls the `from_config()` class method on `HuberLoss` class: this method is implemented by the base class (Loss) and creates an instance of the class, passing `**config` to the constructor. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d0423b-7983-41c8-a3b8-52126e5b14f8",
   "metadata": {},
   "source": [
    "That's it for losses! Similar for activation functions, initializers, regularizers and constraints. Let's look at these now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64c0242-be57-4ace-9ceb-4806ae51990a",
   "metadata": {},
   "source": [
    "### Custom Activation Functions, Initializers, Regularizers and Constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be01ae6b-8779-4e94-b025-d64377ff6bc7",
   "metadata": {},
   "source": [
    "Most of the Keras functionalities, such as losses, regularizers, constraints, initializers, metrics, activation functions, layers, and even full models, can be customized in very much same way. Most of the time, we just need to write a simple function with appropriate inputs and outputs.\n",
    "\n",
    "Here are some examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25c391a-5f03-4a4b-a301-7e1a71fcf8fa",
   "metadata": {},
   "source": [
    "Custom activation function `softplus`:\n",
    "\n",
    "$$\n",
    " Y = log(1 + e^x)\n",
    "$$\n",
    "\n",
    "`softplus` is smooth continuous version of ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "84a0f86b-e0ef-4cef-9233-bcadb8102169",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_softplus(z):\n",
    "    return tf.math.log(tf.exp(z) + 1.0) # same as tf.nn.softplus(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abd608f-acb2-4895-bee1-5d6d5eefcaf8",
   "metadata": {},
   "source": [
    "A custom Glorot Initializer:\n",
    "\n",
    "$$\n",
    " \\text{Normal distribution with mean 0 and variance } {\\sigma}^2 = \\frac{1}{fan_{avg}} \\\\\n",
    "  \\text{where, } fan_{avg} = \\frac{fan_{in} + {fan_{out}}}{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c93b9ece-a3af-4865-b837-032e6983c604",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_glorot_initializer(shape, dtype=tf.float32):\n",
    "    stddev = tf.sqrt( 2 / (shape[0]  + shape[1]))\n",
    "    #tf.random.normal outputs random values from a normal distribution\n",
    "    return tf.random.normal(shape=shape, mean=0.0, stddev=stddev, dtype=dtype) # same as keras.initializers.glorot_normal()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f52539-e33d-4124-a9a8-6e6f00d33a0e",
   "metadata": {},
   "source": [
    "A custom $l_1$ regularizer:\n",
    "$$\n",
    "    Penalty = \\lambda \\cdot \\sum_{i=1}^N |w_i|\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "801bda08-cffd-4449-a817-5570cae1c539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_l1_regularizer(weights):\n",
    "    # here lambda = 0.01\n",
    "    \"\"\"\n",
    "    this tf.reduce_sum() will calculate the sum of all the values in the tensor.\n",
    "    For example:\n",
    "    t = [[1,1,1], [1,1,1]]\n",
    "    tf.reduce_sum(t) ==> 6 \n",
    "    \n",
    "    So here, this tf.reduce_sum() acts as sigma (summation)\n",
    "    \"\"\"\n",
    "    return tf.reduce_sum(tf.abs(0.01 * weights)) # eqv to keras.regularizer.l1(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9b988a-4b63-40f9-91b9-cdb2f4100d1c",
   "metadata": {},
   "source": [
    "A custom constraint that ensures weights are all positive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "80af57fc-5307-4721-a8e4-1d7e07ba5a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_positive_weights(weights):\n",
    "    \"\"\"\n",
    "    tf.zeros_like creates a tensor with all elements set to zero\n",
    "    \"\"\"\n",
    "    # return value is same as tf.nn.relu()\n",
    "    return tf.where(weights < 0., tf.zeros_like(weights), weights) # eqv to keras.constraints.nonneg() or tf.nn.relu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729085ac-f372-4e27-9379-7fb16d5529d4",
   "metadata": {},
   "source": [
    "These custom function then can be used normally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3993f82a-75e9-45e9-800b-0c82694d8c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(1, activation=my_softplus, kernel_initializer=my_glorot_initializer,\n",
    "                          kernel_regularizer=my_l1_regularizer, \n",
    "                          kernel_constraint=my_positive_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a28c685-150e-4835-9a56-cf0a8ce26099",
   "metadata": {},
   "source": [
    "The activation function will be applied to the output of this `Dense` layer, and its result will be passed on to the next layer. The layer's weights will be initialized using the value returned by the initializer. At each training step the weights will be passed to the regularization function to compute the regularization loss, which will be added to the main loss to get the final loss used for training. Finally, the constraint function will be called after each training step, and the layer's weights will be replaced by the constrained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bbdb60b8-1748-4ec6-9eb2-789a00c93ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.7512 - mae: 0.9273 - val_loss: inf - val_mae: inf\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6521 - mae: 0.5370 - val_loss: inf - val_mae: inf\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                      input_shape=input_shape),\n",
    "    keras.layers.Dense(1, activation=my_softplus, kernel_initializer=my_glorot_initializer,\n",
    "                          kernel_regularizer=my_l1_regularizer, \n",
    "                          kernel_constraint=my_positive_weights)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[\"mae\"])\n",
    "\n",
    "h = model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c1d5f2e9-7366-4623-b2cd-cbf654326b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_model_with_many_custom_parts.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0e25d0a8-ebb8-423f-9a7c-5bde9d489274",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"my_model_with_many_custom_parts.keras\",\n",
    "                               custom_objects={\n",
    "                                   \"my_l1_regularizer\": my_l1_regularizer,\n",
    "                                   \"my_positive_weights\": my_positive_weights,\n",
    "                                   \"my_glorot_initializer\": my_glorot_initializer,\n",
    "                                   \"my_softplus\": my_softplus\n",
    "                               })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549ad60c-9407-4949-9cb0-edae78f1d4a7",
   "metadata": {},
   "source": [
    "If the function has hyperparameters that need to be saved along with a model, then we will want to subclass the appropriate class, such as `keras.regularizers.Regulizer`, `keras.constraints.Constraint`, `keras.initializers.Initializer` or `keras.layers.Layer` (for any layer including activation functions). Much like we did for custom loss, here's a simple class for $l_1$ regularization that saves its `factor` hyperparameter (this time we do not need to call the parent constructor or the `get_config()` method, as they are not defined by the parent class):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "82a08e70-669b-4575-9d71-8c8eb79e3ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyL1Regularizer(keras.regularizers.Regularizer):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "    def __call__(self, weights):\n",
    "        return tf.reduce_sum(self.factor * tf.abs(weights))\n",
    "    def get_config(self):\n",
    "        return {\"factor\": self.factor}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391be500-004a-48bd-a86e-4750fb8c33c7",
   "metadata": {},
   "source": [
    "Note that we must implement the `call()` method for losses, layers (including activation functions), and models, or the `__call__()` method for regularizers, initializers, and constraints. For metrics, things are bit different, we will see them in a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8e3dd638-844b-4f24-b508-6afdc41e711a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.7632 - mae: 0.9487 - val_loss: inf - val_mae: inf\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6212 - mae: 0.5393 - val_loss: inf - val_mae: inf\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                      input_shape=input_shape),\n",
    "    keras.layers.Dense(1, activation=my_softplus, \n",
    "                       kernel_initializer=my_glorot_initializer,\n",
    "                       kernel_regularizer=MyL1Regularizer(0.01),\n",
    "                       kernel_constraint=my_positive_weights)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[\"mae\"])\n",
    "\n",
    "h = model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "792e9712-a2a7-400d-8bc8-082e9e72f600",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_model_with_many_custom_parts.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "399ca198-1576-4833-8532-1f66c16ae0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"my_model_with_many_custom_parts.keras\",\n",
    "                               custom_objects={\n",
    "                                   \"MyL1Regularizer\": MyL1Regularizer,\n",
    "                                   \"my_positive_weights\": my_positive_weights,\n",
    "                                   \"my_glorot_initializer\": my_glorot_initializer,\n",
    "                                   \"my_softplus\": my_softplus\n",
    "                               })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ceff654-abda-495f-8964-1dcbfd52d0ad",
   "metadata": {},
   "source": [
    "### Custom Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e9029cc8-f8bb-43d7-89d4-09dcda4ce26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                      input_shape=input_shape),\n",
    "    keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a84043d-b1e9-41d0-833e-5de1fb220824",
   "metadata": {},
   "source": [
    "In most cases, defining a custom metric function is exactly the same as defining a custom loss function. In fact, we could even use the Huber loss function we created earlier as a metric; (however, Huber loss is seldom used as metric, MSE or MAE is preferred) it would just work fine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3ef435ce-b303-4fea-9983-fc378a6057c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[create_huber(2.0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5ad01943-2c4c-4111-b1e3-615d7c7dd86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 1s 876us/step - loss: 2.8613 - huber_fn: 1.0737\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 924us/step - loss: 0.8171 - huber_fn: 0.2885\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f90d5e48b10>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cca561-1407-4233-99e9-3be494f6c02d",
   "metadata": {},
   "source": [
    "For each batch during training, Keras will compute this metric and keep track of its mean since the beginning of the epoch. Most of the time, this is exactly what we want. But not always!\n",
    "\n",
    "Consider a binary classifier's precision, for example. As we saw in Chapter 3, precision is the number of true positives, divided by the number of positive predictions (including both true and false positives). Suppose the model made five positive predictions in first batch, four of which were correct: that's 80% prediction. \n",
    "\n",
    "Then suppose model made three positive predictions in the second batch, but they all were incorrect: that's 0% precision for the second batch. If we compute the mean of these two precisions, we get 40%. \n",
    "\n",
    "But wait a second - that's *not* the model's precision over these two batches! Indeed, there were total of 4 true positives (4 + 0) out of 8 positive predictions (5 + 3), so the overall precision is 50% and not 40%. \n",
    "\n",
    "What we need is an object that can keep track of the number of true positives and the number of false positives and that can compute their ratios when requested. \n",
    "\n",
    "This is precisely what the `keras.metrics.Precision` class does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "24e31f61-d817-4556-9c15-dafdae535f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.8>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = keras.metrics.Precision()\n",
    "precision([0, 1, 1, 1, 0, 1, 0, 1], [1, 1, 0, 1, 0, 1, 0, 1]) # labels and predictions for 1st batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "522d1ee7-daf9-4a35-8b99-532f4df6aa33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.5>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision([0, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0, 0]) # labels and predictions for 2nd batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f9984c-6dfd-43e2-83e4-34c330c7e751",
   "metadata": {},
   "source": [
    "In this example, we create a `Precision` object, then we used it like a function, passing it a label and predictions for the first batch, then for the second batch (we could also have passed sample weights). We used the same number of true and false positives as in the example we just discussed. \n",
    "\n",
    "After the first batch, it returns a precision of 80%; then after the second batch, it returns 50% (which is overall precision so far, not the second batch's precision). \n",
    "\n",
    "This is called `streaming metric` or (`stateful metric`), as it is gradually updated, batch after batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ea9169-3513-4669-9e0c-bce300c5c42d",
   "metadata": {},
   "source": [
    "At any point, we can call the `result()` method to get the current value of the metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "73ae1945-f252-4900-9991-f96c724608ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.5>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b90c44-8604-4555-a360-8316121df517",
   "metadata": {},
   "source": [
    "We can also look at its variables (tracking the number of true and false positives) by using the `variables` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "03880b57-64e5-44d9-9de4-940e11bb43cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'true_positives:0' shape=(1,) dtype=float32, numpy=array([4.], dtype=float32)>,\n",
       " <tf.Variable 'false_positives:0' shape=(1,) dtype=float32, numpy=array([4.], dtype=float32)>]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision.variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f1c566-c798-4387-bac2-744a21b6b13c",
   "metadata": {},
   "source": [
    "We can reset these variables using the `rest_states()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "66b88617-e94e-4bef-89ca-a51625e17936",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision.reset_states() # both variables will get reset to 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a2777101-2122-463f-a997-558d142837b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'true_positives:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n",
       " <tf.Variable 'false_positives:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision.variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f1e5e1-17ac-486e-bd34-1fad47813c39",
   "metadata": {},
   "source": [
    "If we need to create such a streaming metric, create a subclass of `keras.metrics.Metric` class. \n",
    "\n",
    "Here's the simple example that keeps track of the total Huber loss and the number of instances seen so far. When asked for result, it returns the ratio, which is simply the mean Huber loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "fa467254-56d5-4cf8-9c70-581c8f88da70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberMetric(keras.metrics.Metric):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        super().__init__(**kwargs) # handles base args (e.g dtype)\n",
    "        self.threshold = threshold\n",
    "        self.huber_fn = create_huber(threshold)\n",
    "        # add_weight adds a new variable to the layer\n",
    "        self.total = self.add_weight(\"total\", initializer=\"zeros\")\n",
    "        self.count = self.add_weight(\"count\", initializer=\"zeros\")\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        \"\"\"\n",
    "        This method is called when we use an instance of this class as function (as we did with\n",
    "        `Precision` object). It updates the variables, given the labels and predictions for one batch\n",
    "        (and sample weights, but in this case we ignore them). \n",
    "        \"\"\"\n",
    "        metric = self.huber_fn(y_true, y_pred)\n",
    "        # assign_add() is to update the variable. Since in tf, direct updation is not allowed\n",
    "        self.total.assign_add(tf.reduce_sum(metric))\n",
    "        # tf.size() => Returns a 0-D Tensor representing the number of elements\n",
    "        self.count.assign_add(tf.cast((tf.size(y_true)), tf.float32))\n",
    "    def result(self):\n",
    "        \"\"\"\n",
    "        Computes and returns the final result, in this case the mean Huber metric over all instances.\n",
    "        When we use the metric as a function, the `update_state()` method gets called first, then the\n",
    "        `result()` method is called, and its output is returned\n",
    "        \"\"\"\n",
    "        return self.total / self.count\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "        Implemented this method to ensure that `threshold` gets saved along with the model. \n",
    "        \"\"\"\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "cc3ff8e4-6c37-4175-b806-2d4ff9d47ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=14.0>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = HuberMetric(2.0)\n",
    "\n",
    "\"\"\"\n",
    "error (10 - 2 = 8) > threshold (2) . Therefore it will act linearly\n",
    " total = 2 * (|10-2| - 2/2) = 14\n",
    " count = 1\n",
    " result = 14 / 1 = 14\n",
    "\"\"\"\n",
    "\n",
    "# y_true, y_pred\n",
    "m(tf.constant([[2.]]), tf.constant([[10.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1f59607c-921e-47a0-b335-269945b47102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=7.0>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total = total + (|1-0|^2 / 2) + (2 * (|9.25 - 5| - 2/2)) = 14 + 0.5 + 6.5 = 21\n",
    "# count = count + 2 = 1 + 2 = 3\n",
    "# result = total / count = 21 / 3 = 7\n",
    "\n",
    "m(tf.constant([[0.], [5.]]) , tf.constant([[1.], [9.25]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4af63c46-86fb-4f49-a583-3750a273b36e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=7.0>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c0a63238-c0b6-4cba-bcab-4d5c669bf750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'total:0' shape=() dtype=float32, numpy=21.0>,\n",
       " <tf.Variable 'count:0' shape=() dtype=float32, numpy=3.0>]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "67897748-f5cc-4276-b0af-ad944367394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "bd4eb83f-0720-47da-96c7-58d585bb4b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'total:0' shape=() dtype=float32, numpy=0.0>,\n",
       " <tf.Variable 'count:0' shape=() dtype=float32, numpy=0.0>]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f2c356-ffc3-4b8b-9a8a-d48a8c89f56e",
   "metadata": {},
   "source": [
    "Let's check that the `HuberMetric` class works well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "dff75c82-3d14-4f71-9c13-9fa129072883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.9816 - huber_metric: 0.9816\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2520 - huber_metric: 0.2520\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\", \n",
    "                      input_shape=input_shape),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=create_huber(2.0), optimizer=\"nadam\", metrics=[HuberMetric(2.0)])\n",
    "\n",
    "h = model.fit(X_train_scaled.astype(np.float32), y_train.astype(np.float32), epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d2309be4-2fe5-4926-933d-ae8897f87093",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_model_with_a_custom_metric.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d06529c1-ba79-4236-a14d-ee0a6c70acf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"my_model_with_a_custom_metric.keras\",\n",
    "                               custom_objects={\n",
    "                                   \"huber_fn\": create_huber(2.0),\n",
    "                                   \"HuberMetric\": HuberMetric\n",
    "                               })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b7bc4b01-f721-4e1c-89ad-f5677359d858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.2299 - huber_metric: 0.2299\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2219 - huber_metric: 0.2219\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f90d450fd90>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled.astype(np.float32), y_train.astype(np.float32), epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2b9606df-c035-42e2-ae06-c08aeeb9f8bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.src.metrics.base_metric.Mean at 0x7f90d6a81dd0>,\n",
       " <__main__.HuberMetric at 0x7f90d46a1ed0>]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6d409c52-15d9-4d6a-8fd0-2f05f831091c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics[-1].threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c85924c-de34-4e06-9469-3f5eb610d076",
   "metadata": {},
   "source": [
    "When we define a metric using a simple function, Keras automatically calls it for each batch, and it keeps track of the mean durin each epoch, just like we did manually. So the only benefit of our `HuberMetric` class is that the `threshold` will be saved. But of course, some metrics like precision, cannot simply be averaged over batches, in those cases, there's no other option than to implement a streaming metric. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb5e07f-65a7-40f2-af00-511ccec16229",
   "metadata": {},
   "source": [
    "More simply, we could have the created the class like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a7024caf-2444-4242-b302-18a472e47dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberMetric(keras.metrics.Mean):\n",
    "    def __init__(self, threshold=1.0, name=\"HuberMetric\", dtype=None):\n",
    "        self.threshold = threshold\n",
    "        self.huber_fn = create_huber(threshold)\n",
    "        super().__init__(name=name, dtype=dtype)\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        metric = self.huber_fn(y_true, y_pred)\n",
    "        super(HuberMetric, self).update_state(metric, sample_weight)\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30be7a0-ff08-46e3-9390-8e77c3db0c7b",
   "metadata": {},
   "source": [
    "This class handles shapes better, and it also supports sample weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "caa091d1-84df-4bee-badf-bce199f807c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3758 - HuberMetric: 0.7602\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.1223 - HuberMetric: 0.2494\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                      input_shape=input_shape),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=keras.losses.Huber(2.0), optimizer=\"nadam\", metrics=[HuberMetric(2.0)])\n",
    "\n",
    "sample_weight = np.random.rand(len(y_train))\n",
    "h = model.fit(X_train_scaled.astype(np.float32), y_train.astype(np.float32),\n",
    "             epochs=2, sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9749e280-a208-4828-bd45-f95c4d7abce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.37584829330444336, 0.37724936285833965)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.history[\"loss\"][0] , h.history[\"HuberMetric\"][0] * sample_weight.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "9ede3c20-f350-4087-8c4f-b42976ac404a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_model_with_a_custom_metric_v2.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "45512559-3285-4eb6-acca-2bc42d4db09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"my_model_with_a_custom_metric_v2.keras\",\n",
    "                               custom_objects={\n",
    "                                   \"HuberMetric\": HuberMetric\n",
    "                               })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "8f8536fc-9cc7-4f29-9eaa-36c0c0a4bd17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.2344 - HuberMetric: 0.2344\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2235 - HuberMetric: 0.2235\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f908c6e7890>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled.astype(np.float32), y_train.astype(np.float32), epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "18a83c1f-e63a-412d-a65a-b2bb74588f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics[-1].threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6e0a0c-9fbf-4aec-9d94-12da0ee365cb",
   "metadata": {},
   "source": [
    "Now we have built a streaming metric, building a custom layer will seem like a walk in park. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18332ce1-db47-4379-9c76-e2cd2aad694e",
   "metadata": {},
   "source": [
    "### Custom Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a369f13f-9d66-4417-98b9-5f035daec7bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (latest)",
   "language": "python",
   "name": "latest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
