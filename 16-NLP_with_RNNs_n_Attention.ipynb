{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85d357cc-b277-400b-9f03-ae7692484855",
   "metadata": {},
   "source": [
    "# Natural Language Processing with RNNs and Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "411a3858-a1f3-4b6d-95cd-c5179ec68d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d94c99-0e0a-419a-82e8-8e50f9fb0d8b",
   "metadata": {},
   "source": [
    "## Generating Shakespearean Text Using a Character RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d3715e-5007-4640-8ade-431983fcc106",
   "metadata": {},
   "source": [
    "In 2015 blog post, Andrej Karpathy showed how RNN can be used to train a model to predict the next character in the sequence. We will look at how to build Char-RNN, step by step, starting with creation of dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc9c406-404c-4fd2-89c6-42db15f2f54a",
   "metadata": {},
   "source": [
    "### Creating the Training Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e41ebb3-9955-49e2-862f-0d87a70ba421",
   "metadata": {},
   "source": [
    "First let's download all of Shakespere's work, using Keras's handy `get_file()` function and downloading the data from Andrej Karpathy's Char-RNN Project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7f5cc83-5b67-4093-9920-2860601d3f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "filepath = keras.utils.get_file(\"shakespere.txt\", shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63adba24-7003-4979-8d73-717e7af08eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(shakespeare_text[:148])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de6ca72b-dae0-4d0c-b53f-3b00910d2056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(sorted(set(shakespeare_text.lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97ca28da-ba9e-450e-b406-a9ceed4bf5ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"\".join(sorted(set(shakespeare_text.lower()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0ca15e-36af-4e3c-9cfc-298143579806",
   "metadata": {},
   "source": [
    "Next, we must encode every character as an integer. One option is to create a custom preprocessing layer as we did in Chapter-13. But in this case, it will be simpler to use Keras's `Tokenizer` class. \n",
    "\n",
    "First we need to fit a tokenizer to the text: it will find all the characters used in the text and map each of them to a different character ID, from the 1 to the number of distinct characters (it does not start at 0, as we can use that value for masking, as we will see later):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e10d736-8a4c-498d-af4a-fee973cc1733",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True) # char_level=True means every character will be treated as a token\n",
    "tokenizer.fit_on_texts([shakespeare_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346ef3b9-21eb-4bee-bf45-c15b997d042a",
   "metadata": {},
   "source": [
    "We set `char_level=True` to get the character-level encoding rather than the default word-level encoding. Note that tokenizer converts the text to lowercase by default (but we can set `lower=False` if we do not want that). Now the tokenizer can encode a sentence (or a list of sentences) to a list of character IDs and back, and it tells how many distinct characters are there and total number of characters in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec78eb46-9492-4d6b-952e-2dcba00ded84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[20, 6, 9, 8, 3]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences([\"First\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d609e645-4553-4de5-934f-e0a2a71f9b7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f i r s t']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([[20,6,9,8,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4712937-9df8-4b83-84b9-cff28be633ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_id = len(tokenizer.word_index) # number of distinct characters\n",
    "max_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c893cccc-871f-4234-8a3d-368972ab71ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_size = sum(tokenizer.word_counts.values()) # total number of characters\n",
    "dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00a5e908-4ea3-452e-9bc1-d69e49c869d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(shakespeare_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd5c7b8-f808-4928-8cba-499a76bf3310",
   "metadata": {},
   "source": [
    "Let's encode the full text so each character is represented by its ID (we subtract 1 to get IDs from 0 to 38, rather than 1 to 39):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db7c0f35-1309-477c-967b-2bf18780c867",
   "metadata": {},
   "outputs": [],
   "source": [
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacdd275-59ca-4c01-9984-3c2999160d95",
   "metadata": {},
   "source": [
    "Before we continue, we need to split the dataset into training, a validation and a test set. We can't just shuffle all the characters in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df45fb3-b763-4312-aed0-fa87820d9c66",
   "metadata": {},
   "source": [
    "### How to split a Sequential Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb6f31d-b8b8-48cb-9f70-56f747f816b4",
   "metadata": {},
   "source": [
    "> Refer notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c3e630-e8ea-41e7-b737-91a1542e4c40",
   "metadata": {},
   "source": [
    "Let's take the first 90% of the text for the training set (keeping the rest for the validation set and the test set), and create a `tf.data.Dataset` that will return each character one by one from this set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aecbdcfb-8144-47da-9adf-9068acf30002",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = dataset_size * 90 // 100\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6018daf-3cd3-41ca-872b-bc629a8150ba",
   "metadata": {},
   "source": [
    "### Chopping the Sequential Dataset into Multiple Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241a7818-0598-4bbd-b4ef-17c703fea6cd",
   "metadata": {},
   "source": [
    "The training set now consists of a single sequence of over a million characters, so we can't just train the neural network directly on it: the RNN would be equivalent to a deep net with over a million layers, and we would have a single (very long) instance to train it. Instead, we will use dataset's `window()` method to convert this long sequence of characters into many smaller windows of text. Every instance in the dataset will be a fairly short substring of the whole text, and the RNN will be unrolled only over the length of these substrings. This is called *truncated backpropogation through time*. \n",
    "\n",
    "Let's call the `window()` method to create a dataset of short text windows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2aaef84f-3177-4c32-a8db-6675f34a9951",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "window_length = n_steps + 1 # target = input shifted 1 character ahead\n",
    "dataset = dataset.window(window_length, shift=1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74984dd0-30dc-43ab-8d67-e370b92d9719",
   "metadata": {},
   "source": [
    "**TIP:**\n",
    "\n",
    "We can try tuning `n_steps`: it is easier to train RNNs on shorter input sequences, but of course the RNN will not be able to learn any pattern longer than `n_steps`, so don't make it too small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6d7bb5-a75c-4a23-848a-de27fc79bd81",
   "metadata": {},
   "source": [
    "By default `window()` method creates a nonoverlapping windows, but to get the largest possible training set we use `shift=1` so that the first window contains characters 0 to 100, the second contains 1 to 101, and so on. To ensure that all the windows are exactly 101 characters long (which will allow to create batches without having to do padding), we set `drop_remainder=True` (otherwise the last 100 windows will contain 100 characters, 99 characters, and so on down to 1 character).\n",
    "\n",
    "The `window()` creates a dataset that contains windows, each of which is also represented as dataset. It is *nested dataset*, analogous to a list of lists. This is useful when we want to transform each window by calling its dataset methods (e.g., to shuffle them or batch them). However, we cannot use a nested dataset directly for training, as our models will expect tensors as input, not datasets. So we must call the `flat_map()` method: it converts nested dataset into *flat dataset* (one that does not contain datasets). Moreover, the `flat_map()` method takes a function as an argument, which allows us to transform each dataset in the nested dataset before flattening. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44ac7b70-0066-4331-95f4-8e8d30e9f8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b533f16-3f1a-4a6f-9176-2bab8b65b4c0",
   "metadata": {},
   "source": [
    "Notice that we call the `batch(window_length)` on each window: since all windows have exactly the same length, we will get a single tensor for each of them. Now the dataset contains consecutive windows of 101 characters each. Since GD works best when the instances in the training set are independet and indentically distributed , we need to shuffle these windows. Then we can batch the windows and seperate the inputs (the first 100 characters) from the target (the last character):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27705eec-166c-4d8c-bc6c-049aa2914185",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[: ,1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf886e1-669e-421d-9f42-16918764bf7b",
   "metadata": {},
   "source": [
    "As discussed in Chapter-13, categorical input features should generally be encoded, usually as one-hot vectors or as embeddings. Here we will encode each character using a one-hot vector because they are fairly few distinct characters (only 39):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de451cdf-2655-45b9-9d41-522d3b18ed5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch)\n",
    ")\n",
    "\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dccaf99c-8c7a-4969-9bcc-d5ebcb854f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 39) (32, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-13 09:37:01.003966: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "for X_batch , Y_batch in dataset.take(1):\n",
    "    print(X_batch.shape, Y_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12391d5-98dc-4d0e-a5e5-22020b1f81fd",
   "metadata": {},
   "source": [
    "That's it! Preparing the dataset was the hardest part. Now let's create the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab275edf-826b-47e2-ba5d-82144661cff8",
   "metadata": {},
   "source": [
    "### Building and Training the Char-RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ee27250-4be6-4257-a34b-0eb10c5998d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"loss\", patience=5)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"char-rnn_model.keras\",\n",
    "                                                      monitor=\"loss\",save_best_only=True)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id], dropout=0.2,\n",
    "                    recurrent_dropout=0.2), # 20% dropout on both the inputs (dropout) and hidden state (recurrent_dropout) ... Remove recurrent_dropout in both current and below layers when training model on GPU\n",
    "    keras.layers.GRU(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=\"softmax\")) # numbers of neurons in dense layer = max_id = 39, because we want to predict next character and since we want to predict the probability of next character we use the \"softmax\" activation function.\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c347c64f-a628-4ed8-816d-6bc9d3436945",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = model.fit(dataset, epochs=15, callbacks=[early_stopping_cb, model_checkpoint_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed84a912-9040-43e5-9e3e-ef7f6d7adf6b",
   "metadata": {},
   "source": [
    "> Model training and inference is done on GPU and its weights have been saved in \"char-rnn_model.keras\" file. Access collab notebook from [here](https://colab.research.google.com/drive/1EeHuSc7t1waluhqLljOPufDPcLbTy39u?usp=sharing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c447ccc-380e-43f7-bcd1-43a12d74dde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"char-rnn_model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce8faef-e316-43f1-ad60-4b1265da0c19",
   "metadata": {},
   "source": [
    "### Using the Char-RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eec1a46-c479-48fe-a953-07f2b58621db",
   "metadata": {},
   "source": [
    "Now we have a model that can predict the next character in text written by Shakespeare. To feed it some text, we first need to preprocess it like we did earlier, so let's create a function for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c304581b-0a70-44f7-b472-0b9d85942841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(texts):\n",
    "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "    # print(tf.one_hot(X, max_id))\n",
    "    return tf.one_hot(X, max_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816d9a14-86e1-4975-9833-8947960bd010",
   "metadata": {},
   "source": [
    "Now let's use the model to predict the next letter in some text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6d5d21-c38d-451b-9744-3bd2c4c9c8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = preprocess([\"How are yo\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c983e4-8a89-4980-8873-b56941745f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = np.argmax(model(X_new), axis=-1)\n",
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a522b28-ac98-439c-9bab-cec7df7b889b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.sequences_to_texts(Y_pred+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afeb695a-100a-4368-bed6-3b3161164404",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.sequences_to_texts(Y_pred+1)[0][-1] # last character"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6b8c95-9308-4da1-8cb1-3bd3adcd348c",
   "metadata": {},
   "source": [
    "Success! The model guessed right. Now let's use this model to generate new text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbaf433-13cb-46dd-bcbf-cfde0efb7352",
   "metadata": {},
   "source": [
    "### Generating Fake Shakespearean Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6aea64-dbb8-4b1c-a3c2-e3b598d3746b",
   "metadata": {},
   "source": [
    "To generate new text using the Char-RNN model, we could feed it some text, make the model predict the most likely next letter, add it at the end of the text, then give the extended text to the model to guess the next letter, and so on. But in practice, it often leads to same words being repeated over and over again. \n",
    "\n",
    "Instead we can pick the next character randomly, with probability equal to the estimated probability using Tensorflow's `tf.random.categorical()` function. This will generate more diverse text. \n",
    "\n",
    "The `categorical()` function samples random class indices, given the class log probabilities (logits). To have more control over the diversity of the generated text, we can divide the logits by a number called *temperature*, which we can tweak as we wish: a temperature close to 0 will favour high-probability characters, while a very high tempeature will give all characters an equal probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4854dc-6b29-4fb5-90d2-7c268f7eced5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "tf.random.categorical([[np.log(0.5), np.log(0.4), np.log(0.1)]], num_samples=5).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c77bc4-6843-4947-9892-64f7456ad297",
   "metadata": {},
   "source": [
    "Let's break it down above code!\n",
    "\n",
    "Imagine you have a bag of colored marbles, and each color represents a different category. Now, let's say you want to randomly pick marbles from this bag, but not all colors are equally likely to be picked. Some colors might be more likely than others.\n",
    "\n",
    "The `tf.random.categorical` function is like a magical machine that helps you do this. It's like asking the machine to pick marbles from the bag according to certain rules.\n",
    "\n",
    "Here's how it works:\n",
    "\n",
    "1. **Input**: You need to tell the machine about the bag of marbles and how likely each color is to be picked. In technical terms, you provide the machine with a list of numbers called \"logits\". These logits represent the probabilities of each category. For example, if you have three colors (red, green, blue), you might tell the machine that there's a 50% chance of picking red, 40% chance of picking green, and 10% chance of picking blue. But you provide these probabilities in a special way using logits.\n",
    "\n",
    "2. **Generate Samples**: Once the machine knows about the probabilities, you ask it to pick marbles from the bag. You tell it how many marbles you want it to pick. In technical terms, you specify the number of samples you want.\n",
    "\n",
    "3. **Output**: After you ask the machine to pick marbles, it gives you a list of numbers back. Each number represents the color of a marble it picked. These numbers are the indices of the categories you defined earlier. For example, if you have three colors and the machine gives you the numbers 0, 1, 0, 2, 1, it means it picked the first color, then the second color, then the first color again, then the third color, and finally the second color again.\n",
    "\n",
    "So, in simple terms, `tf.random.categorical` is like a machine that randomly picks items (categories) from a list (distribution) based on how likely you tell it each item is to be picked. It then gives you back the items it picked in the form of a list of numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7d7fd5-345e-4e5c-9519-3b719fefbe36",
   "metadata": {},
   "source": [
    "The following `next_char()` function uses above mentioned approach to pick the next character to add to the input text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421ff8d7-c6cb-4501-8d36-efcd1f485774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(text, temperature=1):\n",
    "    X_new = preprocess([text])\n",
    "    y_proba = model(X_new)[0, -1:, :] # all columns of last row\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f41a997-29fd-49ac-9210-8fc7e911268e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "next_char(\"how are yo\", temperature=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d01f8c-2be3-44fd-a9ac-ff7a5c49bb11",
   "metadata": {},
   "source": [
    "Now, let's write a small function that will call next_char() to get the next character and applied it to given text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8f1b8e-dece-491f-9c98-ebbb40856150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "    text += next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5adc5c5-567f-4714-bb5d-f9492104eb99",
   "metadata": {},
   "source": [
    "Now we are ready to generate some text! Let's try with different temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56e44c9-4ab6-4b2d-8e08-14f32e36cca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(complete_text(\"t\", temperature=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de089d3a-6755-4e86-996b-fbe76842d78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(complete_text(\"w\", temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36f105c-219a-44d7-b21b-5d851be84c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(complete_text(\"w\", temperature=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c99904-80a1-41ed-8846-6f01ab967d51",
   "metadata": {},
   "source": [
    "Apparently our Shakespeare model works best at temperature close to 1. To generate more convincing text, we could try using more GRU layers and more neurons per layer, training for longer, and add some regularization (for example, we could set `recurrent_dropout=0.3` in `GRU` layers). Moreover, the model is currently incapable of learning patterns longer than `n_steps`, which is just 100 characters. We could try making this window larger, but it will also make training harder and even LSTM and GRU cells cannot handle very long sequences. \n",
    "\n",
    "Alernatively, we could use a stateful RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6033fbb6-0147-4430-b965-b75ee74364cf",
   "metadata": {},
   "source": [
    "### Stateful RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a56087-ad38-47cc-a390-b4659631dfe0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (latest)",
   "language": "python",
   "name": "latest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
