{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85d357cc-b277-400b-9f03-ae7692484855",
   "metadata": {},
   "source": [
    "# Natural Language Processing with RNNs and Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "411a3858-a1f3-4b6d-95cd-c5179ec68d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-11 00:09:24.692676: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-11 00:09:24.734324: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-11 00:09:24.734860: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-11 00:09:25.632369: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d94c99-0e0a-419a-82e8-8e50f9fb0d8b",
   "metadata": {},
   "source": [
    "## Generating Shakespearean Text Using a Character RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d3715e-5007-4640-8ade-431983fcc106",
   "metadata": {},
   "source": [
    "In 2015 blog post, Andrej Karpathy showed how RNN can be used to train a model to predict the next character in the sequence. We will look at how to build Char-RNN, step by step, starting with creation of dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc9c406-404c-4fd2-89c6-42db15f2f54a",
   "metadata": {},
   "source": [
    "### Creating the Training Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e41ebb3-9955-49e2-862f-0d87a70ba421",
   "metadata": {},
   "source": [
    "First let's download all of Shakespere's work, using Keras's handy `get_file()` function and downloading the data from Andrej Karpathy's Char-RNN Project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7f5cc83-5b67-4093-9920-2860601d3f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "filepath = keras.utils.get_file(\"shakespere.txt\", shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63adba24-7003-4979-8d73-717e7af08eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(shakespeare_text[:148])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de6ca72b-dae0-4d0c-b53f-3b00910d2056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(sorted(set(shakespeare_text.lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97ca28da-ba9e-450e-b406-a9ceed4bf5ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"\".join(sorted(set(shakespeare_text.lower()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0ca15e-36af-4e3c-9cfc-298143579806",
   "metadata": {},
   "source": [
    "Next, we must encode every character as an integer. One option is to create a custom preprocessing layer as we did in Chapter-13. But in this case, it will be simpler to use Keras's `Tokenizer` class. \n",
    "\n",
    "First we need to fit a tokenizer to the text: it will find all the characters used in the text and map each of them to a different character ID, from the 1 to the number of distinct characters (it does not start at 0, as we can use that value for masking, as we will see later):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e10d736-8a4c-498d-af4a-fee973cc1733",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True) # char_level=True means every character will be treated as a token\n",
    "tokenizer.fit_on_texts([shakespeare_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346ef3b9-21eb-4bee-bf45-c15b997d042a",
   "metadata": {},
   "source": [
    "We set `char_level=True` to get the character-level encoding rather than the default word-level encoding. Note that tokenizer converts the text to lowercase by default (but we can set `lower=False` if we do not want that). Now the tokenizer can encode a sentence (or a list of sentences) to a list of character IDs and back, and it tells how many distinct characters are there and total number of characters in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec78eb46-9492-4d6b-952e-2dcba00ded84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[20, 6, 9, 8, 3]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences([\"First\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d609e645-4553-4de5-934f-e0a2a71f9b7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f i r s t']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([[20,6,9,8,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4712937-9df8-4b83-84b9-cff28be633ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_id = len(tokenizer.word_index) # number of distinct characters\n",
    "max_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c893cccc-871f-4234-8a3d-368972ab71ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_size = tokenizer.document_count # total number of characters\n",
    "dataset_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd5c7b8-f808-4928-8cba-499a76bf3310",
   "metadata": {},
   "source": [
    "Let's encode the full text so each character is represented by its ID (we subtract 1 to get IDs from 0 to 38, rather than 1 to 39):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db7c0f35-1309-477c-967b-2bf18780c867",
   "metadata": {},
   "outputs": [],
   "source": [
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacdd275-59ca-4c01-9984-3c2999160d95",
   "metadata": {},
   "source": [
    "Before we continue, we need to split the dataset into training, a validation and a test set. We can't just shuffle all the characters in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df45fb3-b763-4312-aed0-fa87820d9c66",
   "metadata": {},
   "source": [
    "### How to split a Sequential Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb6f31d-b8b8-48cb-9f70-56f747f816b4",
   "metadata": {},
   "source": [
    "> Refer notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c3e630-e8ea-41e7-b737-91a1542e4c40",
   "metadata": {},
   "source": [
    "Let's take the first 90% of the text for the training set (keeping the rest for the validation set and the test set), and create a `tf.data.Dataset` that will return each character one by one from this set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aecbdcfb-8144-47da-9adf-9068acf30002",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = dataset_size * 90 // 100\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6018daf-3cd3-41ca-872b-bc629a8150ba",
   "metadata": {},
   "source": [
    "### Chopping the Sequential Dataset into Multiple Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241a7818-0598-4bbd-b4ef-17c703fea6cd",
   "metadata": {},
   "source": [
    "The training set now consists of a single sequence of over a million characters, so we can't just train the neural network directly on it: the RNN would be equivalent to a deep net with over a million layers, and we would have a single (very long) instance to train it. Instead, we will use dataset's `window()` method to convert this long sequence of characters into many smaller windows of text. Every instance in the dataset will be a fairly short substring of the whole text, and the RNN will be unrolled only over the length of these substrings. This is called *truncated backpropogation through time*. \n",
    "\n",
    "Let's call the `window()` method to create a dataset of short text windows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2aaef84f-3177-4c32-a8db-6675f34a9951",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "window_length = n_steps + 1 # target = input shifted 1 character ahead\n",
    "dataset = dataset.window(window_length, shift=1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74984dd0-30dc-43ab-8d67-e370b92d9719",
   "metadata": {},
   "source": [
    "**TIP:**\n",
    "\n",
    "We can try tuning `n_steps`: it is easier to train RNNs on shorter input sequences, but of course the RNN will not be able to learn any pattern longer than `n_steps`, so don't make it too small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6d7bb5-a75c-4a23-848a-de27fc79bd81",
   "metadata": {},
   "source": [
    "By default `window()` method creates a nonoverlapping windows, but to get the largest possible training set we use `shift=1` so that the first window contains characters 0 to 100, the second contains 1 to 101, and so on. To ensure that all the windows are exactly 101 characters long (which will allow to create batches without having to do padding), we set `drop_remainder=True` (otherwise the last 100 windows will contain 100 characters, 99 characters, and so on down to 1 character).\n",
    "\n",
    "The `window()` creates a dataset that contains windows, each of which is also represented as dataset. It is *nested dataset*, analogous to a list of lists. This is useful when we want to transform each window by calling its dataset methods (e.g., to shuffle them or batch them). However, we cannot use a nested dataset directly for training, as our models will expect tensors as input, not datasets. So we must call the `flat_map()` method: it converts nested dataset into *flat dataset* (one that does not contain datasets). Moreover, the `flat_map()` method takes a function as an argument, which allows us to transform each dataset in the nested dataset before flattening. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44ac7b70-0066-4331-95f4-8e8d30e9f8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b533f16-3f1a-4a6f-9176-2bab8b65b4c0",
   "metadata": {},
   "source": [
    "Notice that we call the `batch(window_length)` on each window: since all windows have exactly the same length, we will get a single tensor for each of them. Now the dataset contains consecutive windows of 101 characters each. Since GD works best when the instances in the training set are independet and indentically distributed , we need to shuffle these windows. Then we can batch the windows and seperate the inputs (the first 100 characters) from the target (the last character):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27705eec-166c-4d8c-bc6c-049aa2914185",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[: ,1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf886e1-669e-421d-9f42-16918764bf7b",
   "metadata": {},
   "source": [
    "As discussed in Chapter-13, categorical input features should generally be encoded, usually as one-hot vectors or as embeddings. Here we will encode each character using a one-hot vector because they are fairly few distinct characters (only 39):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de451cdf-2655-45b9-9d41-522d3b18ed5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch)\n",
    ")\n",
    "\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12391d5-98dc-4d0e-a5e5-22020b1f81fd",
   "metadata": {},
   "source": [
    "That's it! Preparing the dataset was the hardest part. Now let's create the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab275edf-826b-47e2-ba5d-82144661cff8",
   "metadata": {},
   "source": [
    "### Building and Training the Char-RNN Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (latest)",
   "language": "python",
   "name": "latest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
